\documentclass[a4paper]{article}

% Expanded on 2022-03-13 at 10:25:07.

\usepackage{../../style}

\title{AICC-2}
\author{Joachim Favre}
\date{Jeudi 10 mars 2022}

\begin{document}
\maketitle

\lecture{6}{2022-03-10}{Using conditional entropy}{
}

\parag{Theorem: Chain rule for entropies}{
    Let $S_1, \ldots, S_n$ be random variables. Then:
    \[H_D\left(S_1, \ldots, S_n\right) = H_D\left(S_1\right) + H_D\left(S_2 | S_1\right) + \ldots + H_D\left(S_n|S_1, \ldots, S_{n-1}\right)\]
}


\parag{Theorem}{
    Let $X$ be an arbitrary random variable. Let $f\left(X\right)$ be a (deterministic) function of $X$. Then:
    \[H\left(f\left(X\right) | X\right) = 0\]
}

\parag{Theorem}{
    Let $S_1, \ldots, S_n$ be discrete  random variables. Then:
    \[H\left(S_1, \ldots, S_n\right) \leq H\left(S_1\right) + \ldots + H\left(S_n\right)\]
    with equality if and only if $S_1, \ldots, S_n$ are independent.
}

\parag{Corollary}{
    We get the two following inequalities:
    \[H\left(X, Y\right) \geq H\left(X\right)\]
    \[H\left(X, Y\right) \geq H\left(Y\right)\]
}

\subsection{Random processes}

\end{document}
