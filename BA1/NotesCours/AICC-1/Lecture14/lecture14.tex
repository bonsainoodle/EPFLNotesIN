\documentclass[a4paper]{article}

% Expanded on 2021-11-03 at 15:13:01.

\usepackage{../../style}

\title{AICC}
\author{Joachim Favre}
\date{Mercredi 03 novembre 2021}

\begin{document}
\maketitle

\lecture{14}{2021-11-03}{Big-Theta and algorithm complexity}{
}

\parag{Power of logarithms}{
    Taking $k = 1$ and $C = \left(\frac{t}{\epsilon}\right)^t$, we find that $\log_2\left(x\right)^t$ is $O\left(x^{\epsilon}\right)$.

    We have to be careful about the constants. If $t = 1000$ and $\epsilon = 0.0001$, then $C = 10^{6}$. So, even though in the long term it would grow slower than $x^{\epsilon}$, it is not the case for values that we would use commonly. In other words, even though the big-$O$ of an algorithm is worse than one of another, does not necessarily means that it is better.
}

\parag{Symmetry}{
    If we know that $f$ is not $O\left(g\right)$, then we can not conclude that $g$ is $O\left(f\right)$.
}

\subsection{Big-$\Omega$ and Big-$\Theta$}
\parag{Big-Omega notation}{
    Let $f$ and $g$ be functions from the set of integers or the set of real numbers to the set of real numbers. We say that $f\left(x\right)$ is $\Omega\left(g\left(x\right)\right)$ if there are constants $C$ and $k$ with $C > 0$ such that
    \[\left|f\left(x\right)\right| \geq C\left|g\left(x\right)\right| \mathspace x > k\]

    \subparag{Remark}{
        This is basically the inverse of the big-$O$ notation. Big-$O$ gives an upper bound of the growth of a function, while big-Omega gives a lower bound.
    }

    \subparag{Terminology}{
        We say ``$f\left(x\right)$ is big-Omega of $g\left(x\right)$''.
    }

    \subparag{Symmetric}{
        $f\left(x\right)$ is $\Omega\left(g\left(x\right)\right)$ if and only if $g\left(x\right)$ is $O\left(f\left(x\right)\right)$.
    }
}

\parag{Bit-Theta notation}{
    Let $f$ and $g$ be functions from the set of integers or the set of real numbers to the set of real numbers.

    The function $f\left(x\right)$ is $\Theta\left(g\left(x\right)\right)$ if $f\left(x\right)$ is $O\left(g\left(x\right)\right)$ and $f\left(x\right)$ is $\Omega\left(g\left(x\right)\right)$.

    \subparag{Terminology}{
        We say that ``$f$ is big-Theta of $g\left(x\right)$'', ``$f\left(x\right)$ is of order $g\left(x\right)$'', or ``$f\left(x\right)$ and $g\left(x\right)$ are of the same order''.
    }

    \subparag{Symmetry}{
        When $f\left(x\right)$ is $\Theta\left(g\left(x\right)\right)$ then also $g\left(x\right)$ is $\Theta\left(f\left(x\right)\right)$.

        It is equivalent to say that $f\left(x\right)$ is $O\left(g\left(x\right)\right)$ and $g\left(x\right)$ is $O\left(f\left(x\right)\right)$.
    }
}

\parag{Polynomials}{
By the theorem we saw in the last lesson, we get that $f\left(x\right) = a_nx^{n} + \ldots + a_0$ is $\Theta\left(x^{n}\right)$.
}

\parag{Little-o}{
    We say that ``$f\left(x\right)$ is $o\left(g\left(x\right)\right)$'' if:
    \[\lim_{x \to \infty} \frac{f\left(x\right)}{g\left(x\right)} = 0\]
}

\parag{Little-$o$ and Big-$O$}{
    If $f\left(x\right)$ and $g\left(x\right)$ are functions such that $f\left(x\right)$ is $o\left(g\left(x\right)\right)$, then $f\left(x\right)$ is $O\left(g\left(x\right)\right)$.

    \subparag{Warning}{
        The reciprocal is not true.
    }
}

\parag{Linearithmic and linear functions}{
    We have that $x$ is $O\left(x\log x\right)$.
}

\parag{Polynomial and exponential functions}{
    We have that $x^d$ is $O\left(b^x\right)$, where $b > 1$ and $d > 0$.
}

\parag{The gap between polynomial and exponential}{
    We know there is a tremendous gasp between the polynomial and the exponential growth, we are looking for a function in between.

    Let's write $x = b^{\log_b\left(x\right)}$. Then, we have $x^{d} = b^{d\log_b\left(x\right)}$.

    So, we can choose $\log_b\left(x\right)^c$ for $c > 1$ as exponent of $b$:
    \[b^{\log_b\left(x\right)^{c}}\]

    This is called ``quasi-polynomial'' and it stands between polynomial and exponential growth.
}

\subsection{Computational complexity}
\parag{Task}{
    Given an algorithm, we want to know how efficient it is for solving a problem given an input of a particular size.
}

\parag{Complexity}{
    There are two types of complexity:

    \begin{enumerate}
        \item Time complexity (how much time it uses)
        \item Space complexity (how much computer memory it uses)
    \end{enumerate}
}

\parag{Time complexity}{
    If the algorithm is sequential, all operations are executed in order and we can say that time complexity is equivalent to the number of operations that are performed.
}

\parag{Determining time complexity}{
    We determine the number of basic operations, such as addition, multiplication, etc. We assume that all operations use a constant time.
}

\parag{Worst-case time complexity}{
    We want to be pessimistic. It gives us an upper bound on the number of operations an algorithm uses to solve a problem with input of a particular size.
}

\subsection{Complexity of searching and sorting algorithms}

\parag{Worst case complexity of linear search}{
    We have a loop which is executed at most $n + 1$ times, so $c\left(n\right) = n$, which is $\Theta\left(n\right)$.
}

\parag{Worst case complexity of binary search}{
    Let's assume that $n = 2^k$ for convenience. So, this loop will be run $k + 1$ times. However, we know that:
    \[n = 2^{k} \implies k = \log_2\left(n\right)\]

    Therefore, this algorithm has a cost function that is $\Theta\left(\log n\right)$. This has a slower growth than linear complexity, so it is faster than linear search (but it needs the array to be sorted before; adding the two costs makes binary search become slower if we will search something only once in the list (but if we search multiple times, then it becomes worth)).
}

\parag{Worst case complexity of bubble sort}{
    The outer loop is executed $n - 1$ times.

    In total, the inner loop is executed
    \[\left(n-1\right) + \left(n-2\right) + \ldots + 1 = \frac{n\left(n-1\right)}{2} \text{ times}\]
    which is a polynomial of degree 2.

    Therefore, we deduce that the time complexity of this algorithm is $\Theta\left(n^2\right)$.
}

\parag{Worst case complexity of insertion sort}{
    We notice that the outer loop is executed $n - 1$ times, the first inner loop is executed $n-1 + \ldots + 1$ times, and the second inner loop is also executed $n - 1 + \ldots + 1$ times.

    So, we deduce that the complexity of this algorithm is $\Theta\left(n^2\right)$, the same as bubble sort.
}

\subsection{Understanding complexity}
\parag{Note}{
    Here is the commonly used terminology for the complexity of algorithms:
    \begin{center}
        \begin{tabular}{|ll|}
            \hline
            \textbf{Complexity}               & \textbf{Terminology}    \\
            \hline
            $\Theta\left(1\right)$            & Constant complexity     \\
            $\Theta\left(\log n\right)$       & Logarithmic complexity  \\
            $\Theta\left(n\right)$            & Linear complexity       \\
            $\Theta\left(n\log n\right)$      & Linearithmic complexity \\
            $\Theta\left(n^b\right)$          & Polynomial complexity   \\
            $\Theta\left(b^n\right)$, $b > 1$ & Exponential complexity  \\
            $\Theta\left(n!\right)$           & Factorial complexity    \\
            \hline
        \end{tabular}
    \end{center}
}

\end{document}
