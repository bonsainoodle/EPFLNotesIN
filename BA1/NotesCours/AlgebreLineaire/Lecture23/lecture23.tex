\documentclass[a4paper]{article}

% Expanded on 2021-12-09 at 08:19:26.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Jeudi 09 décembre 2021}

\begin{document}
\maketitle

\lecture{23}{2021-12-09}{Moindres carrés et début des régressions}{
}

\subsection{Moindres carrés}
\parag{Introduction}{
    Maintenant que nous avons vu plus de notions, nous pouvons revenir à notre motivation de départ: on souhaite trouver $\bvec{x} \in \mathbb{R}^n$ tel que $A \bvec{x} = \bvec{b}$. Cependant, si les colonnes de $A$ sont liées, ou si, par exemple on a $m > n$ (plus d'équations que d'inconnues), il se pourrait qu'aucune solution n'existe. Ainsi, nous voulons trouver une solution qui est la plus proche possible.

    On cherche donc $\bvec{x} \in \mathbb{R}^n$ tel que $A \bvec{x}$ est aussi proche que possible de $\bvec{b} \in \mathbb{R}^m$. En considérant tous les $\bvec{x}$ dans $\mathbb{R}^n$, on peut ``atteindre'' n'importe quel point de $\im\left(A\right)$ (mais aucun autre). Ainsi, puisque $A \bvec{x} = \bvec{b}$ n'a pas de solution, cela implique que $\bvec{b}$ n'est pas dans l'image de $A$. Or, on sait que l'unique point de $\im\left(A\right)$ le plus proche de $\bvec{b}$ est la projection orthogonale de $\bvec{b}$ sur $\im\left(A\right)$. Ainsi, on va tenter de choisir $\bvec{\hat{x}}$ tel que:
    \[A \bvec{\hat{x}} = \proj_{\im\left(A\right)} \bvec{b} = \bvec{\hat{b}}\]

    Comme $\bvec{\hat{b}}$ est dans $\im A$, il existe $\bvec{\hat{x}} \in \mathbb{R}^n$ (qui n'est pas forcément unique) tel que $A \bvec{\hat{x}} = \bvec{\hat{b}}$. Si on trouve un tel $\bvec{\hat{x}}$, on a gagné. En effet, pour tout $x \in \mathbb{R}^n$:
    \[\left\|A \bvec{x} - \bvec{b}\right\| \geq \left\|\bvec{\hat{b}} - \bvec{b}\right\| = \left\|A \bvec{\hat{x}} - \bvec{b}\right\|\]

    En d'autres mots, aucun $\bvec{x}$ ne fait strictement mieux que $\bvec{\hat{x}}$.
}

\parag{Théorème}{
    Étant donné un système d'équation $A \bvec{x} = \bvec{b}$ (qui pourrait ne pas avoir de solution), on définit un autre système d'équations, appelées \important{équations normales}:
    \[A^T A \bvec{\hat{x}} = A^T \bvec{b}\]

    Alors, ce système a au moins une solution, et chaque solution $\bvec{\hat{x}}$ a la propriété que:
    \[\left\|A \bvec{\hat{x}} - \bvec{b}\right\| \leq \left\|A \bvec{x} - \bvec{b}\right\|, \mathspace \forall \bvec{x}\]

    \subparag{Observation}{
        On peut voir que $\left\|A \bvec{\hat{x}} - \bvec{b}\right\|$ est l'erreur à la solution réellement attendue, donc cette erreur est minimale.
    }

    \subparag{Remarque}{
        On appelle chaque $\bvec{\hat{x}}$ une \important{solution au moindres carrés} de $A \bvec{x} = \bvec{b}$ car $\bvec{\hat{x}}$ minimise (rend aussi petit que possible):
        \begin{multiequality}
            \left\|A \bvec{\hat{x}} - \bvec{b} \right\|^2 =\ & \left(A \bvec{\hat{x}} - \bvec{b}\right) \dotprod \left(A \bvec{\hat{x}} - \bvec{b}\right)  \\
            =\ & \left(A \bvec{\hat{x}} - \bvec{b}\right)^2_1 + \ldots + \left(A \bvec{\hat{x}} - \bvec{b}\right)_m^2
        \end{multiequality}

        On minimise donc le carré des erreurs individuelles, ce qui justifie le nom.
    }
}

\parag{Théorème: unicité}{
    Soit $A \in \mathbb{R}^{m \times n}$.

    Les affirmations sont équivalentes:
    \begin{enumerate}
        \item La solution aux moindres carrés de $A \bvec{x} = \bvec{b}$ est unique pour tout $\bvec{b}$.
        \item $A^T A \in \mathbb{R}^{n \times n}$ est inversible.
        \item Les colonnes de $A$ sont linéairement indépendantes.
    \end{enumerate}

    Dans le cas où elles sont vraies (donc qu'au moins une est vraie), alors:
    \[\bvec{\hat{x}} = \left(A^T A\right)^{-1} A^T \bvec{b}\]
}

\parag{Remarque}{
    $A^T A$ est symétrique, et sera toujours symétrique. Si $A^T A$ n'est pas symétrique dans nos calculs c'est que nous avons fait une erreur!
}

\parag{Théorème}{
    Si les colonnes de $A \in \mathbb{R}^{m \times n}$ sont linéairement indépendantes, alors pour tout $\bvec{b} \in \mathbb{R}^m$, la solution aux moindres carrés de $A \bvec{x} = \bvec{b}$ est la solution de:
    \[R \bvec{\hat{x}} = Q^T \bvec{b}\]
    où $A = QR$ est une factorisation $QR$ : $Q^T Q = I_n$ et $R$ est une matrice inversible et triangulaire.
}

\subsection{Régression linéaire}

\parag{Méthode}{
    Mathématiquement, on cherche une droite d'équation:
    \[y = \beta_1 x + \beta_0\]
    où $\beta_1, \beta_0 \in \mathbb{R}$ sont nos inconnues.

    On a des points $\left(x_1, y_1\right), \ldots, \left(x_n, y_n\right)$, et on veut que:
    \[\beta_1 x_i + \beta_0 \approx y_i, \mathspace \forall i = 1, \ldots, n\]

    On veut minimiser l'erreur. Pour commencer, écrivons tout ceci sous forme matricielle:
    \[\begin{bmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} \approx \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix} \implies A \bvec{\beta} \approx \bvec{y}\]

    On cherche donc $\bvec{\beta} \in \mathbb{R}^2$ tel que $A \bvec{\beta}$ soit aussi proche que possible de $\bvec{y}$.
}

\end{document}
