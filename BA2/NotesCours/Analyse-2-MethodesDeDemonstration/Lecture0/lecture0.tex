\documentclass[a4paper]{article}

% Expanded on 2022-02-27 at 19:56:59.

\usepackage{../../style}

\title{Analyse 2 --- Méthodes de Démonstration}
\author{Joachim Favre}
\date{Lundi 14 février 2022}

\begin{document}
\maketitle

\lecture{0}{2022-02-14}{Une liste qui n'en finit pas}{
}

\section[Démonstrations à connaître]{Démonstrations à connaître}

\parag{Théorème: Existence et unicité d'une solution des EDVS}{
    Soit $f: I \mapsto \mathbb{R}$ une fonction continue telle que $f\left(y\right) \neq 0$ pour tout $y \in I$, et soit $g : J \mapsto \mathbb{R}$ une fonction continue.

    \important{Existence:} Alors, pour tout couple $\left(x_0, b_0\right)$ où $x_0 \in J$ et $b_0 \in I$, l'équation
    \[f\left(y\right) y' = g\left(x\right)\]
    admet une solution $y : J' \subset J \mapsto I$ vérifiant la condition initiale.

    \important{Unicité:} Si $y_1 : J_1 \mapsto I$ et $y_2 : J_2 \mapsto I$ sont deux solutions telles que $y_1\left(x_0\right) = y_2\left(x_0\right) = b_0$, alors:
    \[y_1\left(x\right) = y_2\left(x\right), \mathspace \forall x \in J_1 \cap J_2\]

    \subparag{Preuve}{
        Nous allons seulement montrer l'existence de la solution.

        Soit la fonction suivante:
        \[F\left(y\right) = \int_{b_0}^{y} f\left(t\right)dt\]

        On sait que $F\left(y\right)$ est dérivable par le théorème fondamental du calcul intégral. De plus, on sait que $F'\left(y\right) = f\left(y\right) \neq 0$ sur $I$, donc $f\left(y\right)$ ne change pas pas de signe et donc $F\left(y\right)$ est monotone. Puisque $F\left(y\right)$ est continue et monotone, on sait qu'elle est inversible sur $I$.

        Soit aussi la fonction suivante:
        \[G\left(x\right) = \int_{x_0}^{x} g\left(t\right)dt\]

        Par le théorème fondamental du calcul intégral, on sait aussi que $G\left(x_0\right) = 0$ et que $G$ est dérivable sur $J$.

        Définissons aussi la fonction suivante dans un voisinage de $x_0$ (on sait que $F$ est inversible sur $I$, et $F^{-1}\left(G\left(x_0\right)\right) = b_0 \in I$):
        \[y\left(x\right) = F^{-1}\left(G\left(x\right)\right)\]

        Nous allons démontrer que $y\left(x\right)$ est une solution de l'équation $f\left(y\right) y'\left(x\right) = g\left(x\right)$ dans un voisinage de $x_0 \in J$, et qu'elle satisfait $y\left(x_0\right) = b_0$.

        En manipulant notre définition, on obtient que, dans un voisinage de $x_0 \in J$:
        \[F\left(y\left(x\right)\right) = G\left(x\right) \over{\implies}{$\frac{d}{dx}$}  F'\left(y\left(x\right)\right) y'\left(x\right) = G'\left(x\right) \implies f\left(y\right)y'\left(x\right) =  g\left(x\right)\]

        De plus, nous savons par la définition de $G$ et $F$ que $G\left(x_0\right) = 0$ et $F\left(b_0\right) = 0$, donc:
        \[y\left(x_0\right) = F^{-1}\left(G\left(x_0\right)\right) = F^{-1}\left(0\right) = b_0\]

        \qed
    }

    \subparag{Idée de la preuve}{
        Nous partons de notre équation:
        \[g\left(y\right) \frac{dy}{dx} = f\left(x\right)\]

        Et, notre théorème nous dit que c'est plus ou moins équivalent à:
        \[\int f\left(y\right)dy = \int g\left(x\right) dx \iff F\left(y\right) = G\left(x\right)\]
    }
}

\parag{Proposition pour les EDL1}{
    Soient $p, f : I \mapsto \mathbb{R}$ des fonctions continues. Supposons que $v_0 : I \mapsto \mathbb{R}$ est une solution particulière de l'équation suivante:
    \[y'\left(x\right) + p\left(x\right) y\left(x\right) = f\left(x\right)\]

    Alors, la solution générale de cette équation est:
    \[v\left(x\right) = v_0\left(x\right) + Ce^{-P\left(x\right)}, \mathspace \forall C \in \mathbb{R}\]
    où $P\left(x\right)$ est une primitive de $p\left(x\right)$ sur $I$.

    \subparag{Preuve}{
        Nous allons montrer que toute solution de cette équation est de la forme $v_0\left(x\right) + Ce^{-P\left(x\right)}$.

        Soit $v_1\left(x\right)$ une solution de $y'\left(x\right) + p\left(x\right)y\left(x\right) = f\left(x\right)$. On a aussi que $v_0\left(x\right)$ est une solution de la même équation.

        Alors, d'après le principe de superposition de solutions, la fonction $v_1\left(x\right) - v_0\left(x\right)$ est une solution de l'équation:
        \[y'\left(x\right) + p\left(x\right)y\left(x\right) = f\left(x\right) - f\left(x\right) = 0\]

        Ainsi, $v_1\left(x\right) - v_0\left(x\right)$ est une solution de l'équation homogène:
        \[y'\left(x\right) + p\left(x\right) y\left(x\right) = 0\]

        Cependant, c'est une EDVS, donc nous savons que la solution générale de cette équation homogène est:
        \[v\left(x\right) = Ce^{-P\left(x\right)}, \mathspace C \in \mathbb{R} \text{ arbitraire}\]
        où $P\left(x\right)$ est une primitive de $p\left(x\right)$ sur $I$.

        On en déduit qu'il existe une valeur de $C \in \mathbb{R}$ telle que $v_1\left(x\right) - v_0\left(x\right) = Ce^{-P\left(x\right)}$. Ainsi, on obtient que la solution $v_1\left(x\right)$ est de la forme:
        \[v_1\left(x\right) = v_0\left(x\right) + Ce^{-P\left(x\right)}\]

        Puisque $v_1\left(x\right)$ était une solution arbitraire, nous obtenons que l'ensemble de toutes les solutions de l'équation $y'\left(x\right) + p\left(x\right)y\left(x\right) = f\left(x\right)$ est:
        \[v\left(x\right) = v_0\left(x\right) + Ce^{-P\left(x\right)}, \mathspace C \in \mathbb{R}, x \in I\]

        Donc, par définition, $v\left(x\right)$ est la solution générale.

        \qed
    }
}

\parag{Proposition pour le Wronskien}{
    Soient $v_1, v_2 : I \mapsto \mathbb{R}$ deux solutions de l'équation $y''\left(x\right) + p\left(x\right)y'\left(x\right) + q\left(x\right)y\left(x\right) = 0$ (EDL2 homogène).

    $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement indépendants si et seulement si $W\left[v_1, v_2\right]\left(x\right) \neq 0$ pour tout $x \in I$.

    \subparag{Preuve $\impliedby$}{
        Démontrons ce point par la contraposée. Nous voulons donc montrer que les solutions sont linéairement dépendantes implique qu'il existe $x \in I$ tel que $W\left[v_1, v_2\right]\left(x\right) = 0$.

        Puisque nos deux solutions sont linéairement dépendantes, nous pouvons prendre sans perte de généralité qu'il existe $c \in \mathbb{R}$ tel que $v_1\left(x\right) = cv_2\left(x\right)$ (si plutôt $v_2\left(x\right) = c v_1\left(x\right)$, nous pourrions juste échanger les noms, d'où le ``sans perte de généralité'').

        Ainsi, nous avons:
        \begin{multiequality}
            W\left[v_1, v_2\right]\left(x\right) =\ & \det\begin{pmatrix} v_1\left(x\right) & cv_1\left(x\right) \\ v_1'\left(x\right) & cv_1'\left(x\right) \end{pmatrix}  \\
            =\ & cv_1\left(x\right) v_1'\left(x\right) - cv_1\left(x\right) v_1'\left(x\right) \\
            =\ & 0, \mathspace \forall x \in I
        \end{multiequality}

        Nous avons donc trouvé que le Wronskien est nul pour tout $x$ sur cet intervalle, donc il existe bien un $x$ pour lequel il est égal à 0.
    }

    \subparag{Preuve $\implies$}{
        Prouvons aussi cette affirmation par la contraposée. Nous voulons donc montrer que, s'il existe $x_0 \in I$ tel que $W\left[v_1, v_2\right]\left(x_0\right) = 0$, alors $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement dépendantes.

        Puisqu'il existe un tel $x_0 \in I$, nous savons que:
        \[\det\begin{pmatrix} v_1\left(x_0\right) & v_2\left(x_0\right) \\ v_1'\left(x_0\right) & v_2'\left(x_0\right) \end{pmatrix} = 0\]

        Ainsi, le kernel de cette matrice est est non-trivial (il n'est pas de dimension 0), donc il existe un vecteur non nul $\binom{a}{b} \in \mathbb{R}^2$ tel que:
        \[\begin{pmatrix} v_1\left(x_0\right) & v_2\left(x_0\right) \\ v_1'\left(x_0\right) & v_2'\left(x_0\right) \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \]

        Ainsi:
        \[\begin{systemofequations}
                &\ a v_1\left(x_0\right) + b v_2\left(x_0\right) = 0 \\
                &\ a v_1'\left(x_0\right) + b v_2'\left(x_0\right) = 0
            \end{systemofequations}\]

        Soit $v\left(x\right) = av_1\left(x\right) + bv_2\left(x\right)$. Alors, $v\left(x\right)$ est une solution de l'équation donnée par la superposition des solutions. De plus, par le système d'équations que nous venons de trouver, nous avons $v\left(x_0\right) = 0$ et $v'\left(x_0\right) = 0$. Par le théorème de l'existence et unicité d'une solution de l'équation $y''\left(x\right) + p\left(x\right)y'\left(x\right) + q\left(x\right)y\left(x\right) = 0$, cette équation admet une seule solution satisfaisant $y\left(x_0\right) = 0$ et $y'\left(x_0\right) = 0$. Puisque la solution triviale $y\left(x\right) = 0 \ \forall x \in I$ satisfait l'équation et les conditions initiale, alors nécessairement:
        \[v\left(x\right) = av_1\left(x\right) + bv_2\left(x\right) = 0, \mathspace \forall x \in I\]

        Puisque $a$ et $b$ ne sont pas les deux nuls, soit nous avons $v_1\left(x\right) = \frac{-b}{a} v_2\left(x\right)$ pour tout $x \in I$, soit nous avons $v_2\left(x\right) = -\frac{a}{b} v_1\left(x\right)$ pour tout $x \in I$ (soit les deux).

        Nous avons donc bien trouvé que $v_1\left(x\right)$ et $v_2\left(x\right)$ sont linéairement dépendantes sur $I$.

        \qed
    }

    \subparag{Idée de la preuve}{
        On démontre que $Q \implies P$ et $P \implies Q$ par la contraposée car $P$ et $Q$ sont des propositions ``négatives'': il est beaucoup plus simple d'avoir une fonction qui est parfois égale à 0, ou deux fonctions qui sont linéairement dépendantes.
    }
}

\parag{Théorème: Forme des solutions aux EDL2 homogènes}{
    Soient $v_1, v_2 : I \mapsto \mathbb{R}$ deux solutions linéairement indépendantes de l'équation $y''\left(x\right) + p\left(x\right) y'\left(x\right) + q\left(x\right) y\left(x\right) = 0$.

    Alors, la solution générale de cette équation est de la forme:
    \[v\left(x\right) = C_1 v_1\left(x\right) + C_2 v_2\left(x\right), \mathspace C_1, C_2 \in \mathbb{R}, x \in I\]

    \subparag{Preuve}{
        Soit $\widetilde{v}\left(x\right)$ une solution quelconque de l'équation donnée, et soit $x_0 \in I$. Soient aussi $a_0 \in \mathbb{R}$ et $b_0 \in \mathbb{R}$ tels que $\widetilde{v}\left(x_0\right) = a_0$ et $\widetilde{v}'\left(x_0\right) = b_0$.

        Par hypothèse, nous avons deux solutions linéairement indépendantes $v_1, v_2 : I \mapsto \mathbb{R}$. Ainsi, par la caractérisation, nous savons que $W\left[v_1, v_2\right]\left(x\right) \neq 0$ pour tout $x \in I$, ce qui implique que $W\left[v_1, v_2\right]\left(x_0\right) \neq 0$.

        Or, quand le déterminant d'une matrice est non-nul (la matrice est dite \textit{non-dégénérée}), nous savons qu'une équation l'utilisant a une solution unique. Ainsi, nous savons qu'il existe d'uniques constantes $C_1, C_2 \in \mathbb{R}$ telles que:
        \[\begin{systemofequations} C_1 v_1\left(x_0\right) + C_2 v_2\left(x_0\right) = a_0 \\ C_1 v_1'\left(x_0\right) + C_2 v_2'\left(x_0\right) = b_0 \end{systemofequations}\]

        Considérons la fonction $v\left(x\right) = C_1 v_1\left(x\right) + C_2 v_2\left(x\right)$. Nous pouvons voir deux informations. La première est que $v\left(x\right)$ est une solution de l'équation (puisque $v_1\left(x\right)$ et $v_2\left(x\right)$ sont des solutions). La deuxième est que $v\left(x_0\right) = a_0$ et $v'\left(x_0\right) = b_0$.

        Par le théorème de l'existence et unicité d'une solution des EDL2 homogènes satisfaisant des conditions initiales données $v\left(x_0\right)= a_0$ et $v'\left(x_0\right)= b_0$, on a $\widetilde{v}\left(x\right) = v\left(x\right)$ pour tout $x \in I$. Nous avons donc bien montré que notre solution de départ est de la bonne forme.

        \qed
    }
}

\parag{Proposition: Inégalité de Cauchy-Schwarz}{
    Pour tout $\bvec{x}, \bvec{y} \in \mathbb{R}^n$, nous avons:
    \[\left|\left<\bvec{x}, \bvec{y}\right>\right|\leq \left\|\bvec{x}\right\| \cdot \left\|\bvec{y}\right\|\]

    \subparag{Preuve}{
        Soit $\lambda\in \mathbb{R}$. Considérons la somme $\sum_{i=1}^{n} \left(\lambda x_i + y_i\right)^2$.

        Nous savons que $\sum_{i=1}^{n} \left(\lambda x_i + y_i\right)^2 \geq 0$, puisque c'est une somme de termes positifs:
        \[0 \leq \sum_{i=1}^{n} \left(\lambda x_i + y_i\right)^2 = \sum_{i=1}^{n} \left(\lambda^2 x_i^2 + 2x_i y_i + y_i^2\right)\]
        Et donc:
        \[0 \leq \underbrace{\left(\sum_{i=1}^{n} x_i^2\right)}_{a}\lambda^2 + \underbrace{2\left(\sum_{i=1}^{n} x_i y_i\right)}_{b} \lambda + \underbrace{\left(\sum_{i=1}^{n} y_i^2\right)}_{c}, \mathspace \forall \lambda \in \mathbb{R}\]

        Nous avons obtenu une équation quadratique selon $\lambda$ qui est toujours positive. Ainsi, on remarque qu'il est impossible que cette équation ait deux racines, sinon, par le théorèmes des valeurs intermédiaires, elle serait négative en certains points. Nous savons donc qu'elle a un discriminant négatif:
        \[b^2 - 4ac \leq 0 \implies 4\underbrace{\left(\sum_{i=1}^{n} x_i y_i\right)^2}_{= \left<\bvec{x}, \bvec{y}\right>^2} - 4\underbrace{\left(\sum_{i=1}^{n} x_i^2\right)}_{= \left\|\bvec{x}\right\|^2}\underbrace{\left(\sum_{i=1}^{n} y_i^2\right)}_{= \left\|\bvec{y}\right\|^2} \leq 0\]

        Ce qui implique que:
        \[\left\|\bvec{x}\right\|^2 \cdot \left\|\bvec{y}\right\|^2 \geq \left<\bvec{x}, \bvec{y}\right>^2 \implies \left\|\bvec{x}\right\| \cdot \left\|\bvec{y}\right\| \geq \left|\left<\bvec{x}, \bvec{y}\right>\right|\]

        Puisque $\left\|\bvec{x}\right\|$ et $\left\|\bvec{y}\right\|$ sont positifs, nous pouvons enlever leur valeur absolue. Cependant, nous ne pouvons pas enlever celle du produit scalaire, car elle peut être négative (enfin nous pourrions, puisque $\left|x\right| \geq x$, mais nous perdrions de l'information).

        \qed
    }
}

\parag{Théorème: Lien entre les suites dans $\mathbb{R}^n$ et la topologie}{
Un sous-ensemble non-vide $E \subset \mathbb{R}^n$ est fermé si et seulement si toute suite $\left\{\bvec{x_k}\right\} \subset E$ d'éléments de $E$ qui converge a pour limite un élément de $E$.

\subparag{Preuve $\implies$}{
    Nous supposons que $E \subset \mathbb{R}^n$ est fermé.

    Supposons par l'absurde qu'il existe une suite $\left\{\bvec{x_k}\right\} \subset E$ d'éléments de $E$ qui converge et qui a pour limite $\bvec{x} \not\in E$. Ainsi, on sait que $\bvec{x} \in CE$, qui est un ensemble ouvert dans $\mathbb{R}^n$ (puisque $E$ est fermé par hypothèse). Puisque cet ensemble est ouvert nous savons, par définition, que $\exists \delta > 0$ tel que:
    \[B\left(\bvec{x}, \delta\right) \subset CE\]

    Or, cela implique que:
    \[\underbrace{\left\{\bvec{x_k}\ \forall k \in \mathbb{N}\right\}}_{\subset E} \cap \underbrace{B\left(\bvec{x}, \delta\right)}_{\subset CE} = \o\]

    En d'autres mots, aucun élément de la suite ne fait partie de cette boule ouverte.

    De l'autre côté, puisque $\lim_{k \to \infty} \bvec{x_k} = \bvec{x}$, nous avons qu'il existe un $k_0 \in \mathbb{N}$ tel que pour tout $k \geq k_0$:
    \[\bvec{x_k} \in \bar{B\left(\bvec{x}, \frac{\delta}{2}\right)} \subset B\left(\bvec{x}, \delta\right)\]

    En d'autres mots, pour $k \geq k_0$, $\bvec{x_k}$ fait partie de notre boule ouverte. Ceci entre en contradiction avec ce que nous avions vu ci-dessus.

    \qed
}

\subparag{Preuve $\impliedby$}{
Nous allons démontrer notre proposition par la contraposée: nous voulons montrer que si $E \subset \mathbb{R}^n$ n'est pas fermé, alors il existe une suite $\left\{\bvec{x_k} \subset E\right\}$ d'éléments de $E$ qui converge et qui a pour un limite un élément qui n'est pas dans $E$.

Puisque nous savons que $E$ n'est pas fermé, nous savons que $CE$ n'est pas ouvert. Ainsi, $\exists \bvec{y} \in CE$ tel que, pour tout $\epsilon > 0$:
\[B\left(\bvec{y}, \epsilon\right) \cap E \neq \o\]

Plus précisément, on peut prendre $\epsilon = \frac{1}{k}$, ce qui nous donne:
\[\forall k \in \mathbb{N}_+, \ B\left(\bvec{y}, \frac{1}{k}\right) \cap E \neq \o\]

Ceci implique que, pour tout $k$, on sait qu'il existe un $\bvec{y_k}$ tel que $\bvec{y_k} \in B\left(\bvec{y}, \frac{1}{k}\right)$ et $\bvec{y_k} \in E$. Ceci nous donne une suite $\left\{\bvec{y_k}\right\}_{k \in \mathbb{N}_+} \subset E$ telle que $\lim_{k \to \infty} \bvec{y_k} = \bvec{y} \in CE$, et donc $\bvec{y} \not\in E$.

\qed
}
}


% \parag{Théorème: Caractérisation des limites à partir des suites convergentes}{
% Une fonction $f: E \mapsto \mathbb{R}$ définie au voisinage de $\bvec{x_0}$ admet pour limite $\ell \in \mathbb{R}$ lorsque $\bvec{x} \to \bvec{x_0}$ si et seulement si \textit{pour toute} suite d'éléments $\left\{\bvec{a_k}\right\}$ de $\left\{\bvec{x} \in E \telque \bvec{x} \neq \bvec{x_0}\right\}$, qui converge vers $\bvec{x_0}$, la suite $\left\{f\left(\bvec{a_k}\right)\right\}$ converge vers $\ell$.

% En d'autres mots:
% \[\left(\lim_{\bvec{x} \to \bvec{x_0}} f\left(\bvec{x}\right) = \ell\right) \iff \left(\lim_{k \to \infty} f\left(\bvec{a_k}\right) = \ell,\ \forall \left\{\bvec{a_k}\right\} \subset E \setminus \left\{\bvec{x_0}\right\} \text{ telle que } \lim_{k \to \infty} \bvec{a_k} = \bvec{x_0}\right)\]

% \subparag{Preuve $\implies$}{
%     Nous savons par hypothèse que $\lim_{\bvec{x} \to \bvec{x_0}} f\left(\bvec{x}\right) = \ell$. Ainsi, par la définition de la limite, on sait que, pour tout $\epsilon > 0$, il existe $\delta > 0$ tel que:
%     \[0 < \left\|\bvec{x} - \bvec{x_0}\right\| \leq \delta \implies \left|f\left(\bvec{x}\right) - \ell\right| \leq \epsilon\]

%     Soit une suite arbitraire $\left\{\bvec{a_k}\right\} \subset E \setminus \left\{\bvec{x_0}\right\}$ telle que $\lim_{k \to \infty} \bvec{a_k} = \bvec{x_0}$. Puisque la définition des limites pour les suites marche pour tout $\widetilde{\epsilon}$, nous pouvons prendre $\widetilde{\epsilon} = \delta$. Ainsi, par définition, pour $\widetilde{\epsilon} = \delta > 0$, nous savons que $\exists k_0$ tel que, pour tout $k \geq k_0$, on a:
%     \[\left\|\bvec{a_k} - \bvec{x_0}\right\| \leq \delta\]

%     Or, puisque $\left\{\bvec{a_k}\right\} \subset E \setminus \left\{\bvec{x}_0\right\}$, nous savons que $\bvec{a_k} - \bvec{x_0}\neq 0$. Ainsi, pour tout $k \geq k_0$, $0 < \left\|\bvec{a_k} - \bvec{x_0}\right\| \leq \delta$. Cependant, cela implique par la première implication que:
%     \[\left|f\left(\bvec{a_k}\right) - \ell\right| \leq \epsilon\]

%     Ainsi, nous avons démontré que pour tout $\epsilon > 0$, il existe $k_0$ tel que pour tout $k \geq k_0$ on a $\left|f\left(\bvec{a_k}\right) - \ell\right| \leq \epsilon$. En d'autres mots, nous avons montré que:
%     \[\lim_{k \to \infty} f\left(\bvec{a_k}\right) = \ell\]
% }

% \subparag{Preuve $\impliedby$}{
% Nous allons faire cette preuve par la contraposée. Ainsi, nous supposons par hypothèse que $\lim_{\bvec{x} \to \bvec{x_0}} f\left(\bvec{x}\right) \neq \ell$.

% Par la définition de la limite, on obtient que $\exists \epsilon > 0$ tel que $\forall \delta > 0$, $\exists \bvec{x_\delta}$ tel que:
% \[\left\|\bvec{x_{\delta}} - \bvec{x_0}\right\| \leq \delta \mathspace \text{et} \mathspace \left|f\left(\bvec{x_{\delta}}\right) - \ell\right| > \epsilon\]

% Puisque c'est vrai pour tout $\delta$, alors c'est aussi vrai pour le cas particulier où $\delta = \frac{1}{k}$, $k \in \mathbb{N}_+$. Ainsi, pour le $\epsilon$ dont nous connaissons l'existence, pour tout $k \in \mathbb{N}_+$, il existe $\bvec{x_k} \in E$ tel que:
% \[\left\|\bvec{x_k} - \bvec{x_0}\right\| \leq \frac{1}{k} \mathspace \text{et} \mathspace \left|f\left(\bvec{x_k}\right) - \ell\right| > \epsilon\]

% On obtient la suite $\left\{\bvec{x_k}\right\}_{k=1}^{\infty}$ qui est telle que, par la définition, $\lim_{k \to \infty} \bvec{x_k} = \bvec{x_0}$. Cependant, cette suite est aussi telle que $\left|f\left(\bvec{x_k}\right) - \ell\right| > \epsilon$ pour tout $k \in \mathbb{N}_+$, ce qui implique que:
% \[\lim_{k \to \infty} f\left(\bvec{x_k}\right) \neq \ell\]

% \qed
% }
% }

% \parag{Théorème du min et du max sur un compact}{
% Une fonction continue sur un sous-ensemble compact $E \subset \mathbb{R}^2$ atteint son maximum et son minimum, i.e.:
% \[\exists \max_{\bvec{x} \in E} f\left(\bvec{x}\right), \mathspace \exists \min_{\bvec{x} \in E} f\left(\bvec{x}\right)\]

% \subparag{Preuve $f\left(E\right)$ est borné}{
% Nous voulons commencer par montrer que $\left\{f\left(\bvec{x}\right)\right\}_{\bvec{x} \in E}$ est borné.

% Supposons par l'absurde que $f\left(E\right)$ n'est pas borné, c'est à dire que pour tout $k \geq 0$, il existe un $\bvec{x_k} \in E$ tel que $\left|f\left(\bvec{x_k}\right)\right| \geq k$. Ceci nous donne une suite $\left\{\bvec{x_k}\right\} \in E$.

% Puisque $E$ est un ensemble compact, nous savons qu'il est borné, et donc $\left\{\bvec{x_k}\right\}$ est bornée. Ainsi, par le théorème de Bolzano-Weierstrass, nous pouvons trouver une sous suite convergente $\left\{\bvec{x_{k_{p}}}\right\}$, qui a pour limite un vecteur $\bvec{x_0} \in \mathbb{R}^n$. Puisque $E$ est compact (et donc fermé), nous savons que $\bvec{x_0} \in E$.

% Puisque $f$ est continue, nous savons que:
% \[\lim_{p \to \infty} f\left(\bvec{x_{k_{p}}}\right) = f\left(\bvec{x_0}\right) \in\mathbb{R}\]

% Mais, par construction, $\left|f\left(\bvec{x_k}\right)\right| \geq k$ pour tout $k \in \mathbb{N}$, ce qui est notre contradiction. Nous en concluons que $f$ est bornée sur $E$.
% }

% \subparag{Preuve $f$ atteint ses extremum}{
%     Nous voulons montrer que $f$ atteint son minimum et son maximum sur $E$.

%     Par ce que nous venons de démontrer, nous savons que $f\left(E\right)$ est un sous-ensemble borné. Ainsi:
%     \[\exists M = \sup\left\{f\left(\bvec{x}\right), \bvec{x} \in E\right\}, \mathspace \exists m = \inf\left\{f\left(\bvec{x}\right), \bvec{x} \in E\right\}\]

%     Par la définition du supremum et de l'infimum, nous pouvons nous en rapprocher arbitrairement, donc cela implique qu'il existe deux suites $\left\{\bvec{a_k}\right\}, \left\{\bvec{b_k}\right\} \in E$ telles que:
%     \[\lim_{k \to \infty} f\left(\bvec{a_k}\right) = m, \mathspace \lim_{k \to \infty} f\left(\bvec{b_k}\right) = M\]

%     Or, puisque $\left\{\bvec{a_k}\right\}, \left\{\bvec{b_k}\right\} \in E$ (qui est borné), ce sont des suites bornées, et donc il existe des sous-suites convergentes. En d'autres mots:
%     \[\bvec{a_{k_p}} \to \bvec{a} \in \mathbb{R}^n, \mathspace \bvec{b_{k_p}} \to \bvec{b} \in \mathbb{R}^n\]

%     De plus, puisque $E$ est compact (et donc fermé), nous savons que $\bvec{a} \in E$ et $\bvec{b} \in E$. Ainsi, par la continuité de $f$:
%     \[m = \lim_{k \to \infty} f\left(\bvec{a_k}\right) = \lim_{p \to \infty} f\left(\bvec{a_{k_p}}\right) = f\left(\bvec{a}\right)\]
%     \[M = \lim_{k \to \infty} f\left(\bvec{b_k}\right) = \lim_{p \to \infty} f\left(\bvec{b_{k_p}}\right) = f\left(\bvec{b}\right)\]

%     Ainsi, nous savons qu'il existe $\bvec{a}, \bvec{b} \in E$ tels que:
%     \[f\left(\bvec{a}\right) = m = \min_{\bvec{x} \in E} f\left(\bvec{x}\right)\]
%     \[f\left(\bvec{b}\right) = M = \max_{\bvec{x} \in E} f\left(\bvec{x}\right)\]

%     \qed
% }
% }

% \parag{Proposition: Hypothèses équivalentes pour le théorème de la condition suffisante pour un extremum local quand $n = 2$}{
%     Dans le cas où $n = 2$, nous pouvons réécrire les conditions de notre théorème.

%     Notre matrice Hessienne est donnée par:
%     \[\Hess_f\left(\bvec{a}\right) = \begin{pmatrix} \frac{\partial^{2} f}{\partial x^{2}} & \frac{\partial^2 f}{\partial y \partial x} \\ \frac{\partial^2 f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial y^{2}}  \end{pmatrix} = \begin{pmatrix} r & s \\ s & t \end{pmatrix} \]
%     Nous avons les équivalences suivantes:
%     \begin{enumerate}
%         \item $\lambda_1 > 0, \lambda_2 > 0 \iff \det \Hess_f\left(\bvec{a}\right) > 0 \text{ et } r > 0$
%         \item $\lambda_1 < 0, \lambda_2 < 0 \iff \det \Hess_f\left(\bvec{a}\right) > 0 \text{ et } r < 0$
%         \item $\lambda_1 > 0, \lambda_2 < 0 \text{ ou } \lambda_1 < 0, \lambda_2 > 0 \iff \det \Hess_f\left(\bvec{a}\right) < 0$
%     \end{enumerate}

%     \subparag{Preuve}{
%         Pour commencer, nous savons que le déterminant et la trace d'une matrice sont des invariants de conjugaisons. Ainsi, si on a:
%         \[O \begin{pmatrix} r & s \\ s & t \end{pmatrix} O^{-1} = \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} \iff \begin{pmatrix} r & s \\ s & t \end{pmatrix} = O^{-1} \begin{pmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{pmatrix} O\]

%         Alors, on obtient:
%         \[rt - s^2 = \det\Hess_f\left(\bvec{a}\right) = \det\left(O\right)\lambda_1 \lambda_2 \det\left(O^{-1}\right) = \lambda_1 \lambda_2\]
%         \[r + t = \Tr\Hess_f\left(\bvec{a}\right) = \Tr\left(O D O^{-1}\right) = \Tr\left(O^{-1} O D\right) = \Tr\left(D\right) = \lambda_1 + \lambda_2\]
%     }

%     \subparag{Preuve point 1 $\implies$}{
%         Commençons par montrer la direction $\implies$. Ainsi, nous supposons que $\lambda_1 > 0, \lambda_2 > 0$.

%         Alors, clairement, $\det\Hess_f\left(\bvec{a}\right) = \lambda_1 \lambda_2 > 0$. Aussi, nous voyons que:
%         \[\lambda_1 \lambda_2 = rt - s^2 > 0 \implies rt > s^2 \geq 0 \implies rt > 0\]
%         donc $r$ et $t$ sont de même signe.

%         Nous pouvons aussi voir que:
%         \[\Tr\Hess_f\left(\bvec{a}\right) = \underbrace{\lambda_1}_{> 0} + \underbrace{\lambda_2}_{> 0} = r + t > 0\]
%         donc $r$ et $t$ doivent être les deux strictement positifs, puisqu'ils ont le même signe.

%         Nous en déduisons bien que $\det\Hess_f\left(\bvec{a}\right) > 0$ et $r > 0$.
%     }

%     \subparag{Preuve point 1 $\impliedby$}{
%         Supposons que $\det\Hess_f\left(\bvec{a}\right) > 0$ et $r > 0$.

%         Alors, puisque $\det\Hess_f\left(\bvec{a}\right) = \lambda_1 \lambda_2 > 0$, nous en déduisons que $\lambda_1$ et $\lambda_2$ sont de même signe. De plus, nous voyons aussi que $rt > s^2 \geq 0 \implies rt > 0$.

%         Ainsi, puisque $rt > 0$ et $r > 0$, nous obtenons que $t > 0$. De plus, cela implique que:
%         \[\Tr\Hess_f\left(\bvec{a}\right) = \lambda_1 + \lambda_2 = \underbrace{r}_{> 0} + \underbrace{t}_{> 0} > 0\]

%         Puisque $\lambda_1$ et $\lambda_2$ sont de mêmes signes, et $\lambda_1 + \lambda_2 > 0$, nous en déduisons bien que $\lambda_1 > 0$ et $\lambda_2 > 0$.
%     }

%     \subparag{Preuve point 2 $\implies$}{
%         Commençons par montrer la direction $\implies$. Ainsi, nous supposons que $\lambda_1 < 0, \lambda_2 < 0$.

%         Alors, clairement, $\det\Hess_f\left(\bvec{a}\right) = \lambda_1 \lambda_2 > 0$. Aussi, nous voyons que:
%         \[\lambda_1 \lambda_2 = rt - s^2 > 0 \implies rt > s^2 \geq 0 \implies rt > 0\]
%         donc $r$ et $t$ sont de même signe.

%         Nous pouvons aussi voir que:
%         \[\Tr\Hess_f\left(\bvec{a}\right) = \underbrace{\lambda_1}_{< 0} + \underbrace{\lambda_2}_{< 0} = r + t < 0\]
%         donc $r$ et $t$ doivent être les deux strictement négatifs, puisqu'ils ont le même signe.

%         Nous en déduisons bien que $\det\Hess_f\left(\bvec{a}\right) > 0$ et $r < 0$.
%     }

%     \subparag{Preuve point 2 $\impliedby$}{
%         Supposons que $\det\Hess_f\left(\bvec{a}\right) > 0$ et $r < 0$.

%         Alors, puisque $\det\Hess_f\left(\bvec{a}\right) = \lambda_1 \lambda_2 > 0$, nous en déduisons que $\lambda_1$ et $\lambda_2$ sont de même signe. De plus, nous voyons aussi que $rt > s^2 \geq 0 \implies rt > 0$.

%         Ainsi, puisque $rt > 0$ et $r < 0$, nous obtenons que $t < 0$. De plus, cela implique que:
%         \[\Tr\Hess_f\left(\bvec{a}\right) = \lambda_1 + \lambda_2 = \underbrace{r}_{< 0} + \underbrace{t}_{< 0} < 0\]

%         Puisque $\lambda_1$ et $\lambda_2$ sont de mêmes signes, et $\lambda_1 + \lambda_2 < 0$, nous en déduisons bien que $\lambda_1 < 0$ et $\lambda_2 < 0$.
%     }

%     \subparag{Preuve point 3}{
%         Nous voyons que:
%         \[\det\Hess_f\left(\bvec{a}\right) < 0 \iff \lambda_1 \lambda_2 < 0 \iff \lambda_1 \text{ et } \lambda_2 \text{ sont de signes opposés}\]

%     }

%     \subparag{Note personnelle}{
%         La démonstration de ce théorème peut sembler très longue et compliquée, mais elle ne l'est pas! À partir du moment où on sait que le déterminant est donné par $ad - bc$ et que la trace est donnée par la somme des éléments diagonaux, il suffit de poser nos hypothèses et de simplement voir ce que nous pouvons en déduire, en gardant en tête où nous voulons aller.
%     }
% }

% \parag{Théorème: Condition nécessaire pour un extremum sous contrainte quand $n=2$}{
%     Soit l'ensemble $E \subset \mathbb{R}^2$ et soient les fonctions $f, g : E \mapsto \mathbb{R}$ de classe $C^1$. Supposons que $f\left(x, y\right)$ admette un extremum en $\left(a, b\right) \in E$ sous la contrainte $g\left(x, y\right) = 0$, et que $\nabla g\left(a, b\right) \neq \bvec{0}$.

%     Alors, il existe $\lambda \in \mathbb{R}$, appelé le \important{multiplicateur de Lagrange}, tel que:
%     \[\nabla f\left(a, b\right) = \lambda \nabla g\left(a, b\right)\]

%     \subparag{Preuve}{
%         Nous savons que $\nabla g\left(a, b\right) \neq \bvec{0}$,  donc au moins l'une des dérivées partielles est non-nulle. Supposons que $\frac{\partial g}{\partial y}\left(a, b\right) \neq 0$ (le cas $\frac{\partial g}{\partial x}\left(a, b\right) \neq 0$ est similaire).

%         Nous avons $g\left(a, b\right) = 0$ puisque $\left(a, b\right)$ satisfait la contrainte $g\left(x, y\right) = 0$. Ainsi, par le TFI, il existe une fonction $y = h\left(x\right)$ de classe $C^1$ au voisinage de $x = a$ telle que:
%         \[h'\left(x\right) = - \frac{\frac{\partial g}{\partial x}\left(x, h\left(x\right)\right)}{\frac{\partial g}{\partial y}\left(x, h\left(x\right)\right)}, \mathspace \text{avec } g\left(x, h\left(x\right)\right) = 0\]

%         Aussi, pour $\left(x, y\right)$ satisfaisant notre contrainte $g\left(x, y\right) = 0$, nous pouvons remplacer $y = h\left(x\right)$ dans l'expression $f\left(x, y\right)$ pour obtenir une fonction d'une seule variable:
%         \[f\left(x, y\right) \over{=}{si $g(x, y) = 0$}  f\left(x, h\left(x\right)\right)\]

%         Nous savons que les extrema de cette fonction, respectent:
%         \[f'\left(x, h\left(x\right)\right) = \frac{\partial f}{\partial x}\left(x, h\left(x\right)\right) + \frac{\partial f}{\partial y}\left(x, h\left(x\right)\right)h'\left(x\right) = 0\]

%         Par hypothèse, $\left(a, b\right)$ est un point d'extremum, et il respecte la contrainte $g\left(a, b\right) = 0$, donc les hypothèses de l'équation que nous venons d'obtenir sont bien respectées, ce qui nous permet de trouver que:
%         \[\frac{\partial f}{\partial x}\left(a, b\right) = -\frac{\partial f}{\partial y}\left(a, b\right) h'\left(a\right)\]

%         Pour résumer, nous avons trouvé jusque là que:
%         \[\frac{\partial f}{\partial x}\left(a, b\right) = -\frac{\partial f}{\partial y}\left(a, b\right) h'\left(a\right), \mathspace h'\left(a\right) \over{=}{TFI} -\frac{\frac{\partial g}{\partial x}\left(a, b\right)}{\frac{\partial g}{\partial y}\left(a, b\right)}\]

%         Ceci implique que:
%         \[\underbrace{\frac{\partial f}{\partial x}\left(a, b\right)}_{v_1} = \underbrace{\frac{\partial f}{\partial y}\left(a, b\right)}_{v_2} \frac{\overbrace{\frac{\partial g}{\partial x}\left(a, b\right)}^{u_1}}{\underbrace{\frac{\partial g}{\partial y}\left(a, b\right)}_{u_2 \neq0}}\]

%         Séparons notre preuve en différents cas. Si $u_1 = 0$, alors $v_1 = 0$ et donc $\nabla f\left(a, b\right) = \left(0, v_2\right)$ et $\nabla g\left(a, b\right) = \left(0, u_2\right)$. Ceci implique bien qu'il existe un $\lambda \in \mathbb{R}$ tel que $v_2 = \lambda \underbrace{u_2}_{\neq 0}$ et donc:
%         \[\nabla f\left(a, b\right) = \lambda \nabla g\left(a, b\right)\]

%         Sinon (si $u_1 \neq 0$), alors, en définissant $\frac{v_1}{u_1} = \frac{v_2}{u_2} := \lambda \in \mathbb{R}$, nous trouvons:
%         \[\left(v_1, v_2\right) = \lambda\left(u_1, u_2\right) \iff \nabla f\left(a, b\right) = \lambda \nabla g\left(a, b\right)\]

%         \qed
%     }

%     \subparag{Intuition de la preuve}{
%         Nous trouvons $f\left(x, y\right)$ sous la forme d'une fonction d'une seule variable et la dérivons, puis nous utilisons le théorème des fonctions implicites, ce qui nous permet de trouver un lien entre les dérivées de $f$ et celles de $g$.
%     }
% }

\end{document}
