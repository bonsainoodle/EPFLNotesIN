\documentclass[a4paper]{article}

% Expanded on 2021-12-08 at 15:18:41.

\usepackage{../../style}

\title{AICC 1}
\author{Joachim Favre}
\date{Mercredi 08 d√©cembre 2021}

\begin{document}
\maketitle

\lecture{24}{2021-12-08}{Soon rich at a casino (by making one ofc)}{
}

\section{Probabilities}
\subsection{Introduction and history}

\parag{Definitions}{
    An \important{experiment} is a procedure that yields one \important{outcome} out of a given set. The \important{sample space}, $S$, of the experiment is the set of possible outcomes. An \important{event} $E$ is a subset of the sample space. An event \important{occurs} if the outcome belongs to the event.
}

\parag{Definition of probability}{
    If $S$ is a finite sample space of equally likely outcomes, and $E$ is an event (so a subset of $S$), then the \important{probability} of $E$ is:
    \[p\left(E\right) = \frac{\left|E\right|}{\left|S\right|}\]
}

\parag{Terminology}{
    Note that, in English, we say ``one die'' (singular), and ``multiple dice'' (plural).
}

\subsection{Properties of probabilities}
\parag{Theorem: complements of events}{
    Let $E$ be an event in a sample space $S$. The probability of the event $\bar{E} = S \setminus E$, the \important{complementary events} of $E$, is given by:
    \[p\left(\bar{E}\right) = 1 - p\left(E\right)\]
}


\parag{Theorem: probability of unions}{
    Let $E_1$ and $E_2$ be events in a sample space $S$. Then:
    \[p\left(E_1 \cup E_2\right) = p\left(E_1\right) + p\left(E_2\right) - p\left(E_1 \cap E_2\right)\]
}

\subsection{Probabilities without equally likely outcomes}

\parag{Definition: Probability distribution}{
    Let $S$ be a sample space of an experiment with a finite or countable number of outcomes. We assign a probability $p\left(s\right)$ to each outcome $s$, so that:
    \begin{enumerate}
        \item $0 \leq p\left(s\right) \leq 1$, for each $s \in S$
        \item $\displaystyle \sum_{s \in S}^{} p\left(s\right) = 1$
    \end{enumerate}

    The function $p$ from the set of all outcomes of the sample space $S$ is called a \important{probability distribution}.
}


\parag{Definition: Uniform distribution}{
    Let $S$ be a set with $n$ elements. The \important{uniform distribution} assigns the probability $\frac{1}{n}$ to each elements of $S$, i.e:
    \[p\left(s\right) = \frac{1}{n}, \mathspace \forall s \in S\]
}

\parag{Definition: Probability of an event}{
    The \important{probability} of the event $E$ is the sum of the probabilities of the outcomes in $E$. In other words:
    \[p\left(E\right) = \sum_{s \in E}^{} p\left(s\right)\]
}

\parag{Complements and unions}{
    The two following properties still hold with our new definition:
    \[p\left(\bar{E}\right) = 1 - p\left(E\right), \mathspace p\left(E_1 \cup E_2\right) = p\left(E_1\right) + p\left(E_2\right) - p\left(E_1 \cap E_2\right)\]
}

\subsection{Conditional probability}

\parag{Definition: conditional probability}{
    Let $E$ and $F$ be events, with $p\left(F\right) \neq 0$. The \important{conditional probability} of $E$ given $F$, denoted by $p\left(E | F\right)$, is defined as:
    \[p\left(E|F\right) = \frac{p\left(E \cap F\right)}{p\left(F\right)}\]

    It can be interpreted as the probability that $E$ occurs, give the fact (or knowing) that $F$ occurs.
}

\parag{Definition: Independence}{
    The events $E$ and $F$ are \important{independent} if and only if:
    \[p\left(E \cap F\right) = p\left(E\right)p\left(F\right)\]
}

\parag{Theorem: Independence}{
    If $E$ and $F$ are independent, then:
    \[p\left(E | F\right)= p\left(E\right)\]
}

\parag{Example 1}{
    Let's assume that each of the four ways a family can have two children $\left\{BB, GG, BG, GB\right\}$ (where $B$ means boy, $G$ means girl, in a world where there are exactly two sexes) is equally likely.

    Let $E = \left\{BG, GB\right\}$ be the event that a family with tow children has both girls and boys, and $F = \left\{GG, BG, GB\right\}$ that a family with two children has at most one boy. We wonder if $E$ and $F$ are independent.

    We can deduce that $p\left(E\right) = \frac{1}{2}$ and $p\left(F\right) = \frac{3}{4}$. Moreover, $E \cap F = \left\{BG, GB\right\}$, so $p\left(E \cap F\right) = \frac{1}{2}$. Thus:
    \[p\left(E\right) p\left(F\right) = \frac{3}{8} \neq \frac{1}{2} = p\left(E \cap F\right)\]

    So, those two events are not independent.
}

\parag{Note}{
    Increasing the size of the sample space can change the dependence of events. This means that we must not trust our intuition.
}

\parag{Definition: pairwise independence}{
    The events $E_1, \ldots, E_n$ are \important{pairwise independent} if and only if:
    \[p\left(E_i \cap E_j\right) = p\left(E_i\right)p\left(E_j\right), \mathspace \forall i, j, i < j \leq n\]
}


\parag{Mutual independence}{
    The events $E_1, \ldots, E_n$ are \important{mutually independent} if and only if:
    \[p\left(E_1 \cap \ldots \cap E_n\right) = p\left(E_1\right)\cdot p\left(E_2\right) \cdots p\left(E_n\right)\]
}

\parag{Theorem: Independence}{
    If events are mutually independent, then they are pairwise independent.
}

\end{document}
