\documentclass[a4paper]{article}

% Expanded on 2021-12-14 at 08:17:14.

\usepackage{../../style}

\title{AICC}
\author{Joachim Favre}
\date{Mardi 14 d√©cembre 2021}

\begin{document}
\maketitle

\lecture{25}{2021-12-14}{Bayes-ed}{
}

\parag{Probability trees}{
    We can use trees to analyse complex events which use conditional probabilities.
}

\subsection{Bernoulli trials}

\parag{Definition: Bernoulli trials}{
    Let's say we have an experiment that can have only two possible outcomes, one is called a \important{success} and the other a \important{failure}. If $p$ is the probability of success and $q$ the probability of failure, then we have $p + q = 1$, since those are our two only possible outcomes. Each performance of the experiment is called a \important{Bernoulli trial}.
}

\parag{Theorem}{
    The probability of exactly $k$ successes in $n$ independent Bernoulli trials, with probability of success $p$ and probability of failure $q = 1 - p$ is:
    \[C\left(n, k\right) p^k q^{n-k}\]
}

\parag{Binomial Distribution}{
    We denote $b\left(k:n, p\right)$ the probability of $k$ successes in $n$ independent Bernoulli trials with probability of success $p$. Viewed as a function of $k$, $b\left(k:n, p\right)$ is the \important{binomial distribution}.

    By the theorem above:
    \[b\left(k: n, p\right) = C\left(n, k\right)p^k q^{n-k}\]
}


\subsection{Bayes' Theorem}

\parag{Bayes' Theorem}{
    Let $E$ and $F$ be events from a sample space $S$ such that $p\left(E\right) \neq 0$ and $p\left(F\right) \neq 0$. Then:
    \[p\left(F|E\right) = \frac{p\left(E|F\right)p\left(F\right)}{p\left(E\right)} = \frac{p\left(E|F\right)p\left(F\right)}{p\left(E|F\right)p\left(F\right) + p\left(E | \bar{F}\right)p\left(\bar{F}\right)}\]
}

\parag{Generalised Bayes' theorem}{
    Suppose that $E$ is an event from a sample space $S$ such that $p\left(E\right) \neq 0$ and that $F_1, F_2, \ldots, F_n$ are mutually exclusive events such that:
    \[\bigcup_{i=1}^n F_i = S\]

    Then:
    \[p\left(F_j|E\right) = \frac{p\left(E|F_j\right)p\left(F_j\right)}{p\left(E\right)} = \frac{p\left(E | F_j\right)p\left(F_j\right)}{\sum\limits_{i=1}^{n} p\left(E|F_i\right)p\left(F_i\right)}\]
}

\subsection{Random variables}

\parag{Definition: random variables}{
    A \important{random variable} $X$ is a fonction $X : S \mapsto \mathbb{R}$ from the sample space $S$ of an experiment to the set of real numbers $\mathbb{R}$.

    Note that random variables have nothing to do with variables, and they are not random; they are functions.
}

\end{document}
