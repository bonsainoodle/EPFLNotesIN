\documentclass[a4paper]{article}

% Expanded on 2022-03-17 at 11:42:39.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 17 mars 2022}

\begin{document}
\maketitle

\lecture{8}{2022-03-17}{Let's play games}{
}

\parag{Fundamental theorem of Source Coding}{
    For the compression of a source $S_1, S_2, \ldots$ the fundamental limit is:
    \[\lim_{n \to \infty} \frac{H_D\left(S_1, \ldots, S_n\right)}{n} = H_D^*\left(\mathcal{S}\right)\]
    if this limit exists (if it does not, then we do not know anything).

    In other words, there exist uniquely decodable codes which average codeword-length per symbol gets arbitrarily close to $H^*_D\left(\mathcal{S}\right)$ (such as the Huffman code), and no uniquely decodable code can go below.
}


\parag{Compressing data}{
    We want to know what happens to the entropy when we compress data.

    There is a one-to-one correspondance between $\widetilde{S}^n$, the $n$ characters string encoded using the direct encoding, and $Z^k$, the $k$ characters string (where $k \leq n$ since Huffman codes are optimal) encoded using the Huffman code; if we know one, we can get the others (they are both uniquely decodable code). Thus, necessarily $H\left(\widetilde{S}^n\right) = H\left(Z^k\right)$. However, since the Huffman code is shorter on expectation, then it must have higher entropy per symbol. This is why when we compress data, entropy per symbol goes up.
}

\subsection{Entropy and algorithms}
\parag{Twenty questions}{
    See Joachim's notes.
}

\parag{Sorting}{
    See Joachim's notes.
}

\parag{Billiard balls}{
    See Joachim's notes.
}

\end{document}
