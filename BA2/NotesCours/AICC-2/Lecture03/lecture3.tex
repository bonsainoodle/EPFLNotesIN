\documentclass[a4paper]{article}

% Expanded on 2022-03-03 at 16:07:07.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 01 mars 2022}

\begin{document}
\maketitle

\lecture{3}{2022-03-01}{Kraft paper (wow such humour)}{
}

\subsection{Source coding}

\parag{Definition}{
    The \important{encoder} is specified by the input alphabet $\mathcal{A}$, the output alphabet $\mathcal{D}$ (typically $\mathcal{D} = \left\{0, 1\right\}$ since computers love binary), the codebook $\mathcal{C}$ which consists of finite sequences over $\mathcal{D}$, by the one-to-one encoding map $\Gamma : \mathcal{A}^k \mapsto C$, where $k$ is a positive integer (for now we will take $k = 1$).
}

\parag{Definition: uniquely decodable}{
    The code is \important{uniquely decodable} if every concatenation of codewords has a unique parsing into a sequence of codewords.

    \subparag{Observation 2}{
        A code is uniquely decodable if and only if its reverse (the reverse of 100 is 001) is uniquely decodable.
    }
}

\parag{Note}{
    Every fixed-length code is always uniquely decodable.
}

\parag{Definition: Prefix-free codes}{
    If no codeword is a prefix of another codeword, the code is said to be \important{prefix-free}.

    \subparag{Observation}{
        We can see that a prefix-free code is ($\implies$) uniquely decodable. Since no codeword is the prefix of another, we always know which codeword we are considering (we are not hesitating between 01 or 011 in $\Gamma_C$ for example).
    }

    \subparag{Terminology}{
        A prefix-free code is also called \important{instantaneous code}.
    }
}

\parag{Complete trees}{
    Let's consider all binary sequences of length less than or equal to $\ell_{max}$. We can draw a tree representing it, and then our code in it:
    \imagehere[0.2]{CompleteTreeA.png}

    We can do that for any $D$-ary sequences of length less than or equal to $\ell_{max}$.
}

\parag{Decoding trees}{
    We can remove nodes that are not necessary to make a decoding tree. For example:
    \imagehere[0.35]{DecodingTreeB.png}
}

\parag{Definition: Codeword length}{
    We define the \important{codeword length} to be the size of the codeword (such a beautiful definition).

    We want the average codeword length to be as small as possible.
}

\parag{Theorem: Kraft-McMillan part 1}{
    If a $D$-ary code is uniquely decodable, then its codeword lengths $\ell_1, \ldots, \ell_M$ satisfy:
    \[D^{-\ell_1} + \ldots + D^{-\ell_M} \leq 1\]

    This is called the Kraft inequality.

    \subparag{Contrapositive}{
        If a $D$-ary code's codeword lengths do no satisfy $D^{-\ell_1} + \ldots + D^{-\ell_M} \leq 1$, then this code is not uniquely decodable.
    }

    \subparag{Converse}{
        The converse of this theorem is not true.
    }

}

\parag{Theorem: Kraft-McMillan part 2}{
    If the positive integers $\ell_1, \ldots, \ell_M$ satisfy Kraft's inequality for some positive integer $D$, there exists a $D$-ary prefix-free code (hence uniquely decodable) that has codeword lengths $\ell_1, \ldots, \ell_M$.

    \subparag{Observation}{
        Note that this theorem looks like the converse of the previous one, however it is weaker than it. The converse of the Kraft-McMillan theorem part 1 is wrong.
    }
}

\parag{Summary}{
    We know that if a $D$-ary code is uniquely decodable, then its codeword lengths satisfy Kraft's inequality. We also know that if positive integers satisfy Kraft's inequality, then there exists a $D$-ary prefix-free code that has those codeword lengths.

    Those two imply that any uniquely decodable code can be substituted by a prefix-free code of the same codeword lengths.
}

\end{document}
