\documentclass[a4paper]{article}

% Expanded on 2021-11-02 at 08:35:32.

\usepackage{../../style}

\title{AICC 1}
\author{Joachim Favre}
\date{Mardi 02 novembre 2021}

\begin{document}
\maketitle

\lecture{13}{2021-11-02}{$O$-MG there's Alan}{
}

\subsection{The Halting Problem (yay Turing)}
\parag{Question}{
    Can we solve any problem by an algorithm? Turing (\smiley) proved that nope!

    It is similar to Gödel proving that not every theorem can be proven, with Gödel's incompleteness theorem.
}

\parag{Halting problem}{
    He defined an unsolvable problem, the \important{halting problem}: Can we develop a procedure that takes as input a computer program along with its input and determines whether the program will eventually halt when used with that input?
}

\parag{Theorem}{
    The halting problem cannot be solved using an algorithm.
}

\begin{filecontents*}[overwrite]{collatz.code}
    define function f(n) = n/2 if n is even or 3n + 1 if n is odd
    procedure collatz(n):
    c := f(n)
    while c != 1
    c := f(c)
    return 0
\end{filecontents*}

\section{Measuring complexity}

\parag{Big-$O$ notation}{
    Let $f$ and $g$ be functions from the set of integers or the set of real numbers to the set of real numbers. We say that $f\left(x\right)$ is $O\left(g\left(x\right)\right)$ if there are constants $C$ and $k$ such that:
    \[\left|f\left(x\right)\right| \leq C\left|g\left(x\right)\right|, \mathspace \forall x > k\]

    This is read as ``$f\left(x\right)$ is big-$O$ of $g\left(x\right)$'' or, ``$g$ asymptotically dominates $f$''.

    The constants $C$ and $k$ are called \important{witnesses} to the relationship ``$f\left(x\right)$ is $O\left(g\left(x\right)\right)$''. We only need to find one pair of witnesses to prove the relation.
}

\parag{Theorem (Big-$O$ for polynomials)}{
    Let $f\left(x\right) = a_n x^{n} + \ldots + a_1 x + a_0$, where $a_0, \ldots, a_n$ are real numbers and $a_n \neq 0$. Then, $f\left(x\right)$ is $O\left(x^{n}\right)$.
}

\parag{Monomials}{
    Let $n, m$ constants, with $n > m$. We have that $x^{m}$ is $O\left(x^{n}\right)$ but $x^{n}$ is not $O\left(x^{m}\right)$.
}

\parag{Logarithms}{
    Let $a, b, n, m$ be constants, with $a > 0$, $b >0$ and $n > m$.

    $\log_b\left(x^{m}\right)$ is $O\left(\log_a\left(x^{n}\right)\right)$ and $\log_a\left(x^{n}\right)$ is $O\left(\log_b\left(x^{m}\right)\right)$. Indeed:
    \[\log_b\left(x^{m}\right) = m \log_b\left(x\right) = \frac{m}{\log\left(b\right)} \log\left(x\right)\]

    More specifically, and more importantly, they are all $O\left(\log\left(x\right)\right)$. For a logarithm, its basis and the power inside do not matter.
}

\parag{Factorial function}{
    $n!$ is $O\left(n^{n}\right)$ taking $C = 1$ and $k = 1$.

    Moreover, we see that
    \[\log\left(n!\right) \leq \log\left(n^{n}\right) = n\log\left(n\right)\]

    Hence, we deduce that $\log\left(n!\right)$ is $O\left(n\log\left(n\right)\right)$ taking $C = 1$ and $k = 1$.
    This is a big-$O$ that is often found, which grows a bit faster than linearly. Since it is a combination of a logarithmic and a linear function, we call it linearithmic. It's considered good for an algorithm.
}

\parag{Combinations of functions}{
    \begin{itemize}[left=0pt]
        \item It is transitive. In other words, if $f\left(x\right)$ is $O\left(g\left(x\right)\right)$ and $g\left(x\right)$ is $O\left(h\left(x\right)\right)$ then $f\left(x\right)$ is $O\left(h\left(x\right)\right)$.
        \item It preserves multiplication. In other words, if $f_1\left(x\right)$ is $O\left(g_1\left(x\right)\right)$ and $f_2\left(x\right)$ is $O\left(g_2\left(x\right)\right)$, then $\left(f_1\cdot f_2\right)\left(x\right)$ is $O\left(\left(g_1\cdot g_2\right)\left(x\right)\right)$.
        \item We can add functions that have the same growth without changing anything. In other words, if $f_1\left(x\right)$ and $f_2\left(x\right)$ are both $O\left(g\left(x\right)\right)$, then $\left(f_1 + f_2\right)\left(x\right)$ is also $O\left(g\left(x\right)\right)$.
        \item More precisely, if we add two functions that have different growth, then only the one with the highest matters (just like black lives, they matter). In other words, if $f_1\left(x\right)$ is $O\left(g_1\left(x\right)\right)$ and $f_2\left(x\right)$ is $O\left(g_2\left(x\right)\right)$, then $\left(f_1 + f_2\right)\left(x\right)$ is $O\left(\max\left(\left|g_1\left(x\right)\right|, \left|g_2\left(x\right)\right|\right)\right)$.
    \end{itemize}
}

\parag{Summary}{
    Let's take functions $f : \mathbb{R}_+ \mapsto \mathbb{R}$. We have the following order of growth:
    \begin{description}
        \item[Constant:] $O(1)$
        \item[Logarithmic:] $O\left(\log x\right)$
        \item[Poly-logarithmic:] $O\left(\left(\log x\right)^{d}\right)$, where $d > 1$
        \item[Linear:] $O\left(x\right)$
        \item[Linearithmic:] $O\left(x\log x\right)$
        \item[Polynomial:] $O\left(x^{d}\right)$, where $d > 1$
        \item[Exponential:] $O\left(b^{x}\right)$, where $b > 0$
        \item[Factorial:] $O\left(x^{x}\right)$
    \end{description}

    Until polynomial growth, it is still acceptable. However, exponential growth or more is very bad for an algorithm.
}

\end{document}
