\documentclass[a4paper]{article}

% Expanded on 2022-06-02 at 11:40:42.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 02 juin 2022}

\begin{document}
\maketitle

\lecture{27}{2022-06-02}{Summary}{
}

\parag{Decoding}{
    Let's assume that we receive $\bvec{y}$, which was not modified by the channel, and we want to decode it. We can use a Lagrange interpolation to find $\bvec{u}$ back.

    Note that if the channel made a modification of $\bvec{y}$, it is harder to decode it, but we have nice algorithms to do so (which will not be presented here).
}


\parag{Generator matrix for RS codes}{
    If we have a list of all the elements, we can make a generator matrix by picking $k$ codewords, preferably such that it gives us a systematic generator matrix (if the code is systematic). Then, we can find the parity-check matrix easily if we found our systematic form. However, we would like to find a better way to make a generator matrix.

    When we proved that Reed-Solomon Codes are linear, we proved that the mapping $\bvec{u} \mapsto \bvec{c_n}$ is linear:
    \[\left(\alpha \bvec{u} + \bvec{v}\right) \mapsto \left(\alpha \bvec{c_u} + \bvec{c_v}\right)\]

    This means that if we have a basis for the vectors $\bvec{u}$, we can use it to find a basis for the vectors $\bvec{c}$. We can just use the canonical basis:
    \begin{center}
        \begin{tabular}{c|c|c}
            $\bvec{u}$                                      & $p_{\bvec{u}}\left(x\right)$ & $\bvec{c}$                                    \\
            \hline
            $\bvec{e}_{k} = \left(1, 0, \ldots, 0\right)$   & $x^{k-1}$                    & $\left(a_1^{k-1}, \ldots, a_n^{k-1}\right)$   \\
            $\bvec{e}_{k-1} = \left(0, 1, \ldots, 0\right)$ & $x^{k-2}$                    & $\left(a_1^{k-2}, \ldots, a_{n}^{k-2}\right)$ \\
            $\vdots$                                        & $\vdots$                     & $\vdots$                                      \\
            $\bvec{e}_1 = \left(0, 0, \ldots, 1\right)$     & 1                            & $\left(1, \ldots, 1\right)$
        \end{tabular}
    \end{center}

    But since the mapping is linear, this is also a basis for $\mathcal{C}$, giving us the following generator matrix (which is sadly not necessarily systematic):
    \[G = \begin{pmatrix} 1 & 1 & \cdots & 1 \\ a_1 & a_2 & \cdots & a_n \\ \vdots & \vdots & \ddots & \vdots \\ a_{1}^{k-1} & a_2^{k-1} & \cdots & a_n^{k-1} \end{pmatrix} \]

    This is sometimes the definition of the Reed-Solomon codes.

    \subparag{Remark}{
        Note that the matrix we got is named the Vandermonde matrix, and it shows up in many contexts.
    }
}

\section{Summary}

\subsection{Source Coding}
\parag{Definition: Entropy}{
    Entropy is defined as:
    \[H_D\left(s\right) = -\sum_{s}^{} p\left(s\right) \log_D\left(p\left(s\right)\right)\]

    This is a fundamental definition in science. In this course, we usually pick $D = 2$.
}

\parag{Theorem: Entropy bounds}{
    Entropy has the following bound:
    \[0 \leq H_D\left(\mathcal{S}\right) \leq \log_D \left|\mathcal{S}\right|\]

    In other words, it is bounded by the degenerate probability distribution (one event has probability 1, and all others have probability 0), and by the uniform distribution (every event has the same probability to show up).
}

\parag{Entropy conditioned by event}{
    We saw that, when conditioning over an event, entropy works the same way:
    \[H\left(S | Y = y\right) = -\sum_{s}^{} p\left(s|Y=y\right) \log\left(p\left(s | Y = y\right)\right)\]
}

\parag{Definition: Conditional entropy}{
    Conditional entropy is defined to be:
    \[H\left(S|Y\right) = \sum_{y}^{} p\left(y\right) H\left(S | Y=y\right) = -\sum_{s}^{} \sum_{y}^{} p\left(s, y\right) \log\left(p\left(s|y\right)\right)\]
}

\parag{Theorem: Conditioning reduces entropy}{
    If someone tells us something more, then it reduces the entropy:
    \[H\left(S | Y\right) \leq H\left(S\right)\]

    Note that this is not true for entropy conditioned by an event, under certain conditions we can have $H\left(S | Y=y\right) > H\left(s\right)$. Indeed, under some $y$, $S$ may be way more random. However, its average has to be less random (thus $H\left(S |Y\right) \leq H\left(S\right)$).
}

\parag{Theorem: Chain rule for entropy}{
    Conditional entropy follows the following equality:
    \[H\left(S_1, S_2\right) = H\left(S_1\right) + H\left(S_2 | S_1\right)\]
}

\parag{Source coding}{
    Let us consider a source alphabet $\mathcal{A}$, and a mapping $\Gamma\left(s\right)$. This mapping is our code. The length $\ell\left(s\right)$ gives us the length of the codeword $\Gamma\left(s\right)$.

    We say that a code is uniquely decodable if, when receiving all codewords concatenated, we can decode what was the original content.

    A code is prefix-free if no codeword is the prefix of another, meaning that we can write the code as a tree, where codewords are at leafs.
}

\parag{Theorem}{
    There exists a uniquely decodable code with lengths $\left\{\ell_1, \ell_2, \ldots\right\}$ if and only if there exists a prefix-free code with the same lengths.
}

\parag{Theorem: Kraft inequality}{
    For any $D$-ary uniquely decodable code, the following inequality holds:
    \[\sum_{s \in \mathcal{A}}^{} D^{-\ell\left(s\right)} \leq 1\]
}

\parag{Source model: IID source}{
    We worked over IID sources, meaning that data comes from a fixed distribution, in an independent way (the $n$\Th character does not depend on the characters that came before).

}

\parag{Shannon-Fano code}{
    Over an IID source, we can create a Shannon-Fano code by picking the lengths to be:
    \[\ell \left(s\right) = \left\lceil -\log\left(p\left(s\right)\right) \right\rceil \]

    Note that Shanno-Fano codes are not optimal, but Huffman codes are.
}


\parag{Theorem}{
    For a Shannon-Fano code or a Huffman code, we have:
    \[H_D\left(\mathcal{S}\right) \leq L\left(S, \Gamma\right) \leq H_D\left(\mathcal{S}\right) + 1\]
    where $L\left(S, \Gamma\right)$ is the average codeword length.
}

\parag{Fundamental theorem of source coding}{
    Now, we make Shannon-Fano codes or Huffman code on blocks of length $n$. This gives us:
    \[H^*\left(S\right) = \lim_{n \to \infty} \frac{H\left(S_1, \ldots, S_n\right)}{n}\]
    where $H^*\left(S\right)$ is the entropy rate of the source. This is to what the codeword length $\frac{L\left(S, \Gamma\right)}{n}$ tends towards for Shanno-Fano and Huffman codes when we increase the block size $n$ to infinity.
}

\subsection{Cryptography}
\parag{One-time pad}{
    We have a plaintext $t \in \left\{0, 1\right\}^n$ and a key $k \in \left\{0, 1\right\}^n$ which bits are picked independently and uniformly. Then, we pick the cyphertext to be $c = t \oplus k$.

    This is perfectly secret. In other words, the ciphertext holds no information over the text, we absolutely need to know the key. The thing is, it is very expensive, and only used in very rare scenarios.
}

\parag{Diffie-Hellman}{
    Alice and Bob agree on a $p$ and a $g$ publicly. Then, Alice generates a $a$ she keeps secret and publishes $A = g^a$. Similarly, Bob generates a $b$ he keeps secret, and publishes $B = g^b$. Then, both of them can compute the same common key by doing $K = B^a = A^b = g^{ab}$. Nobody else can get this key, since discrete logs are very hard to do.

    \svghere{DiffieHellmanSystem.svg}

    Then, we can use El Gamal cryptosystem to send a text using this key.
}

\parag{RSA}{
    Bob generates two prime numbers $p$ and $q$, $m = pq$, an encoding exponent $e$, and a decoding exponent $d$. He then publishes $\left(m, e\right)$, which Alice can use to send $c = \left[t^e\right]_m$. Then, Bob can decipher this by doing $t = \left[c^d\right]_m$.

    Nobody else than Bob can compute $t$ since discrete logs and prime factorisations are very hard to do.

    \svghere{RSACryptosystem.svg}

    We needed to do a lot of work to show that there always exist a $d$ such that, for any $t$, we can get back $t$. Also, we needed $m = pq$ to be a product of primes in order to hide the size of the group, which is $\phi\left(m\right) = \left(p-1\right)\left(q-1\right)$ and can only be computed by knowing the prime factorisation of $m$.
}

\subsection{Channel coding}
\parag{Models}{
    We made two models. The first one is the error channel, which flips some bits randomly and uniformly in the message. The second one is the erasure channel, which replaces some symbol by a new symbol ``?''.
}

\parag{Channel coding}{
    The idea is, instead of sending any $\bvec{x} \in \mathbb{F}^n$, we restrict to a subset $\mathcal{C} = \left\{\bvec{c_1}, \ldots, \bvec{c_M}\right\} \subseteq \mathbb{F}^n$.
}

\parag{Channel decoding}{
    To decode, we use the minimum distance decoding. In other words, we pick the codeword that minimises the Hamming distance with what we received.

    Hence, we were interested in $d_{min}\left(\mathcal{C}\right)$.
}

\parag{Remark}{
    In fact, when doing channel coding, we do not want to make a code that has a $k$ as big as the amount of data we want to send. Thus, we split our data in blocks of $k$ bits, and encode every block. When decoding, we split the data we receive into blocks of $n$ bits and decode them one by one.
}

\parag{Theorem: Singleton bound}{
    Any code follow the following inequality:
    \[d_{min}\left(\mathcal{C}\right) \leq n - k + 1\]

    A code for which there is an equality is named a Maximum Distance Separable (MDS) code.
}

\parag{Linear codes}{
    We restricted ourselves to linear codes, since it allows us to find properties more easily. To describe a linear code, we can use a generator matrix $G \in \mathbb{F}^{k \times n}$:
    \[\mathcal{C} = \left\{\bvec{c} \in \mathbb{F}^n : \bvec{c} = \bvec{u} G, \text{ for a } \bvec{u} \in \mathbb{F}^k\right\}\]

    We can also express our code by having a parity-check matrix $H \in \mathbb{F}^{\left(n-k\right) \times n}$:
    \[\mathcal{C} = \left\{\bvec{c} \in \mathbb{F}^n : \bvec{c} H^T = \bvec{0}\right\}\]

    Note that $H$ is a parity matrix for $G$ if and only if $H$ is full rank (meaning $\rank\left(H\right) = n -k$) and $H$ satisfies $GH^T = 0$.
}

\parag{Theorem}{
    For a linear code, we have that:
    \[d_{min}\left(\mathcal{C}\right) = \min_{\bvec{c} \neq \bvec{0}} w\left(\bvec{c}\right)\]
    where $w\left(\bvec{x}\right) = d\left(\bvec{x}, \bvec{0}\right)$ is the Hamming weight.
}

\parag{Syndrome}{
    For a linear code, we can decode a received $\bvec{y}$ using its syndrome. We compute its syndrome $\bvec{s} = \bvec{y} H^T$, look at a table in which we stored all cosets leader and their syndrome, fetch the coset leader $\bvec{t}$ having the same syndrome as $\bvec{y}$, and finally decode by doing:
    \[\bvec{x} = \bvec{y} - \bvec{c}\]
}

\parag{Reed-Solomon codes}{
    We construct a Reed-Solomon by taking all polynomials $p_{\bvec{u}}\left(x\right)$ with degree $\deg\left(p_{\bvec{u}}\left(x\right)\right) \leq k-1$ and by computing:
    \[\bvec{c} = \left(p_{\bvec{u}}\left(a_1\right), \ldots, p_{\bvec{u}}\left(a_n\right)\right)\]
    for fixed $a_1, \ldots, a_n$.
}

\end{document}
