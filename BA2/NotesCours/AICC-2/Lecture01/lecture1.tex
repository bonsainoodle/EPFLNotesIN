\documentclass[a4paper]{article}

% Expanded on 2022-02-22 at 15:14:58.

\usepackage{../../style}

\title{AICC-2}
\author{Joachim Favre}
\date{Mardi 22 f√©vrier 2022}

\begin{document}
\maketitle

\lecture{1}{2022-02-22}{Back to probabilities}{
}

\section{Source coding}
\subsection{Discrete probabilities}
\parag{Definitions}{
    The \important{sample space}, $\Omega = \left\{\omega_1, \ldots, \omega_n\right\}$, is the set of all possible outcomes. An \important{event} $E$ is a subset of $\Omega$.
}

\parag{Independent events}{
    We say that two event $E$ and $F$ are \important{independent} if and only if:
    \[p\left(E | F\right) = p\left(E\right)\]
}


\parag{Disjoint events}{
    We say that two events $E_1$ and $E_2$ are \important{disjoint} if $E_1 \cap E_2 = \o$. Then, we have the following property:
    \[p\left(E_1 \cup E_2\right) = p\left(E_1\right) + p\left(E_2\right)\]

    We note that, if two events are disjoint, then there are definitely not independent. Indeed, knowing that an event happened means that the other event did not happen.
}

\parag{Law of Total probability}{
    For any event $F \subseteq \Omega$ and its complement $F^C$, we have the following equivalence:
    \[p\left(E\right) = p\left(E|F\right)p\left(F\right) + p\left(E|F^C\right)p\left(F^C\right)\]

    More generally, if $\Omega$ is the union of disjoint event $F_1, \ldots, F_n$, then:
    \[p\left(E\right) = p\left(E|F_1\right)p\left(F_1\right) + \ldots + p\left(E|F_n\right)p\left(F_n\right)\]
}

\parag{Bayes' Rule}{
    Let's say we know $p\left(E\right), p\left(F\right)$ and $p\left(E | F\right)$, and we want $p\left(F | E\right)$. We can use the following relation:
    \[p\left(F | E\right) = \frac{p\left(E |F\right)p\left(F\right)}{p\left(E\right)}\]
}

\parag{Random variable}{
    We define \important{random variables} to be functions $X : \Omega \mapsto \mathbb{R}$.

    In other words, they map every event to a real number.
}

\parag{Probability distribution of a random variable}{
    $p_X\left(x\right)$ is the probability that $X = x$, i.e $p\left(E\right)$ where:
    \[E = \left\{\omega \in \Omega : X\left(\omega\right) = x\right\}\]

    We know how to compute such events, so we can compute $p_X\left(x\right)$:
    \[p_X\left(x\right) = \sum_{\omega \in E}^{} p\left(\omega\right)\]
}

\parag{Intervals}{
    We wonder what is the probability that $X \in \left[a, b\right]$. We can compute it in the following way:
    \[\sum_{\omega \in G}^{} p\left(\omega\right), \mathspace \text{where } G = \left\{\omega \in \Omega : X\left(\omega\right) \in \left[a, b\right]\right\}\]

    We can also do it using $p_X$:
    \[\sum_{x \in\left[a, b\right]}^{} p_X\left(x\right)\]
}

\parag{Two random variables}{
    Let $X : \Omega \mapsto \mathbb{R}$ and $Y : \Omega \mapsto \mathbb{R}$ be two random variables. In other words, we map every event to two real lines.

    We want to find the probability of an event $E_{\left(x, y\right)} = \left\{\omega \in \Omega : X\left(\omega\right) = x \text{ and } Y\left(\omega\right) = y\right\}$, denoted $p_{X, Y}\left(x, y\right)$:
    \[p_{X, Y}\left(x, y\right) = \sum_{\omega \in E_{\left(x, y\right)}}^{} p\left(\omega\right)\]

    We can compute $p_X$ from $p_{X, Y}$:
    \[p_X\left(x\right) = \sum_{y}^{} p_{X, Y}\left(x, y\right)\]

    We call $p_X$ the \important{marginal distribution} of $p_{X, Y}\left(x, y\right)$ with respect to $x$. $p_Y$ can be computed similarly.
}

\parag{Expected value}{
    The \important{Expected Value} $\exval\left[X\right]$ of a random variable $X : \Omega \mapsto \mathbb{R}$ is given by:
    \[\exval\left[X\right] = \sum_{\omega}^{} X\left(\omega\right)p\left(\omega\right) \]

    This is basically a weighted mean.
}

\parag{Theorem}{
    We can compute an expected value of a random variable $X : \Omega \mapsto \mathbb{R}$ the following way:
    \[\exval\left[X\right] = \sum_{x}^{} xp_X\left(x\right)\]
}


\parag{Theorem: linearity of expectations}{
    Expectation is a linear operation. In other words, let $X_1, \ldots, X_n$ be random variables and $\alpha_1, \ldots, \alpha_n$ be scalars. Then:
    \[\exval\left[\sum_{i=1}^{n} \alpha_i X_i\right] = \sum_{i=1}^{n} \alpha_i \mathbb{E}\left[X_i\right]\]
}

\end{document}
