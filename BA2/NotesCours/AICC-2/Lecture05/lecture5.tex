\documentclass[a4paper]{article}

% Expanded on 2022-03-08 at 15:30:32.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Mardi 08 mars 2022}

\begin{document}
\maketitle

\lecture{5}{2022-03-08}{Conditional entropy}{
}

\parag{Ternary Huffman Code}{
    We observe that a ternary tree ``must'' have an odd number of leaves. More generally, in the Huffman strategy, we reduce the alphabet by $D - 1$ symbols every step. Moreover, at the end, we want exactly $D$ symbols, meaning we need $D + k\left(D - 1\right)$ symbols to begin with, where $k \in \mathbb{Z}$. Thus, we can fake it by adding a bunch of symbols having probability 0. Doing this again with our tree:
}

\subsection{Conditional entropy}

\parag{Iid strings}{
    Iid strings are strings of independent and identically distributed symbols.
}

\parag{Not iid strings}{
    Let's consider strings that are not iid: if $a$ has probability $\frac{2}{3}$, then $aa$ has not necessary probability $\frac{4}{9}$. For example, in French and in English, after a $q$, it is very probable to have a $u$.
}

\parag{Theorem}{
    The per-letter average codeword-length of a $D$-ary Shannon-Fano code for the random variable $\left(S_1, \ldots, S_n\right)$ fulfills:
    \[\frac{H_D\left(S_1, \ldots, S_n\right)}{n} \leq \frac{L\left(\left(S_1, \ldots, S_n\right), \Gamma_{SF}\right)}{n} < \frac{H_D\left(S_1, \ldots, S_n\right)}{n} + \frac{1}{n}\]
}

\parag{Definition}{
    The conditional expectation of $X$ given $Y = y$ is defined as:
    \[\exval\left[X |Y = y\right] \over{=}{def} \sum_{x \in X}^{} xp_{X|Y}\left(x | y\right)\]
}


\parag{Definition: Condition entropy}{
    The conditional entropy of $X$ given $Y = y$ is defined as:
    \[H_D\left(X|Y=y\right) \over{=}{def} \sum_{x \in \mathcal{X}}^{} p_{X|Y}\left(x|y\right) \log_D\left(p_{X|Y}\left(x|y\right)\right)\]
}

\parag{Theorem}{
    The conditional entropy of a discrete random variable $X \in \mathcal{X}$conditioned on $Y = y$ satisfies:
    \[0 \leq H_D\left(X | Y = y\right) \leq \log_D\left|\mathcal{X}\right|\]
    with equality on the left if and only if the probability distribution is degenerate (there is 100\% chance of an event happening and 0\% for the others) and equality on the right if and only if the probability distribution is uniform.
}

\parag{Definition}{
    The \important{conditional entropy} of $X$ given $Y$ is defined as:
    \[H_D\left(X|Y\right) \over{=}{def} \sum_{y \in \mathcal{Y}}^{} p_Y\left(y\right) H\left(X|Y=y\right) = \sum_{y \in \mathcal{Y}}^{} p_Y\left(y\right) \left[-\sum_{x \in \mathcal{X}}^{} p_{X|Y}\left(x|y\right) \log_D\left(p_{X|Y}\left(x|y\right)\right)\right]\]
    This definition is really important.

    \subparag{Equivalent definition}{
        Note that, as usual, we can define this conditional entropy using expected values:
        \[H_D\left(X|Y\right) = \exval\left[\log_D\left(\frac{1}{p_{X|Y}\left(X|Y\right)}\right)\right]\]

        This can be useful to prove theorems.
    }
}

\parag{Theorem}{
    The conditional entropy of a discrete random variable $X \in \mathcal{X}$ conditioned on $Y$ satisfies:
    \[0 \leq H_D\left(X|Y\right) \leq \log_D\left|\mathcal{X}\right|\]
    with equality on the left if and only if for every $y$ there exists an $x$ such that $p_{X|Y}\left(x|y\right) = 1$, and with equality if and only if $p_{X|Y}\left(x|y\right) = \frac{1}{\left|\mathcal{X}\right|}$ for all $x$ and $y$.
}

\parag{Theorem (conditioning reduces entropy)}{
    For any two discrete random variables $X$ and $Y$, we have:
    \[H_D\left(X|Y\right) \leq H_D\left(X\right)\]
    with equality if and only if $X$ and $Y$ are independent random variables.
}

\parag{Theorem: Chain rule for entropy}{
    Let $X$ and $Y$ be two random variables. Then:
    \[H_D\left(X, Y\right) = H_D\left(X\right) + H_D\left(Y|X\right)\]

    We already knew it if they were independent, but this result is completely general.
}

\end{document}
