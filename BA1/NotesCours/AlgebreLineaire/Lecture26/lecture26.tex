\documentclass[a4paper]{article}

% Expanded on 2021-12-20 at 13:20:56.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Lundi 20 décembre 2021}

\begin{document}
\maketitle

\lecture{26}{2021-12-20}{Calcul de la SVD}{
}

\parag{Cas général}{
    Soit $A \in \mathbb{R}^{m\times n}$ une matrice quelconque.

    \begin{itemize}[left=0pt]
        \item On sait par le théorème spectral que la matrice symétrique $A^T A$ de taille $n \times n$ a $n$ valeurs propres réelles $\lambda_1, \ldots, \lambda_n$ associées à une base de vecteurs propres orthonormées $\bvec{v}_1, \ldots, \bvec{v}_n \in \mathbb{R}^{n}$. On choisit d'ordonner les valeurs propres (on sait qu'elles sont réelles par le théorème spectral; si elles étaient complexes on ne pourrait pas les ordonner) de sorte que:
              \[\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n\]

        \item Les valeurs propres sont positives ou nulles car $\lambda_i = \left\|A \bvec{v}_i\right\|^2$ (on l'a prouvé). On définit donc:
              \[\sigma_1 = \left\|A \bvec{v}_1\right\| = \sqrt{\lambda_1}, \mathspace \ldots, \mathspace \sigma_n = \left\|A \bvec{v}_n\right\| = \sqrt{\lambda_n}\]

              Puisqu'on a ordonné nos valeurs propres, on a aussi la relation d'ordre suivante:
              \[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_n \geq 0\]

              On appelle $\sigma_1, \ldots, \sigma_n$ les \important{valeurs singulières} de $A$.
        \item Il est possible que certaines des valeurs singulières de $A$ soient nulles. Disons que $A$ a $r$ valeurs singulières non nulles:
              \[\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0\]

              En d'autres mots, si $n > r$, alors $\sigma_{r + 1} = \sigma_{r+2} = \ldots = \sigma_n = 0$.
        \item On définit $r$ vecteurs unitaires (c'est-à-dire de norme égale à 1):
              \[\bvec{u}_1 = \frac{1}{\left\|A \bvec{v}_1\right\|} A \bvec{v}_1 = \frac{1}{\sigma_1} A \bvec{v}_1\]
              \[\vdots\]
              \[\bvec{u}_r = \frac{1}{\left\|A \bvec{v}_r\right\|} A \bvec{v}_r = \frac{1}{\sigma_r} A \bvec{v}_r\]

              On a donc:
              \[A \bvec{v}_1 = \sigma_1 \bvec{u}_1, \mathspace \ldots, \mathspace A \bvec{v}_r = \sigma_r \bvec{u}_r\]

              Les vecteurs $\bvec{u}_1, \ldots, \bvec{u}_r$ sont orthonormés dans $\mathbb{R}^m$ car ils sont orthogonaux (on a démontré que $\left(A \bvec{v}_i\right) \dotprod \left(A \bvec{v}_j\right) = 0$ quand $i \neq j$) et car ils sont de norme égale à 1.
        \item Les vecteurs $\bvec{u}_1, \ldots, \bvec{u}_r$ sont linéairement indépendants dans $\mathbb{R}^m$. Donc, on a forcément que $r \leq m$ (par la dimension de $\mathbb{R}^m$). Il reste deux cas possibles:
              \begin{itemize}
                  \item Si $r = m$, alors $\bvec{u}_1, \ldots, \bvec{u}_r$ forment une base orthonormée de $\mathbb{R}^m$.
                  \item Si $r < m$, alors $\bvec{u}_1, \ldots, \bvec{u}_r$ forment une base orthonormée d'un sous-espace de $\mathbb{R}^m$. On sait qu'il est toujours possible de compléter cette base en choisissant $m - r$ vecteurs $\bvec{u}_{r+1}, \ldots, \bvec{u}_m$ tels que:
                        \[\bvec{u}_1, \ldots, \bvec{u}_r, \bvec{u}_{r+1}, \ldots, \bvec{u}_m\]
                        forment une base orthonormée de $\mathbb{R}^m$. En effet, on peut compléter la base avec des vecteurs, puis on peut appliquer le Grand Schtroumpf: il ne fera rien aux $r$ premiers vecteurs puisqu'ils sont déjà orthonormaux, mais va rendre orthonormaux les vecteurs qu'on ajouté.
              \end{itemize}

        \item Dans tous les cas, on a:
              \[A \bvec{v}_i = \sigma_i \bvec{u}_i, \mathspace \forall i = 1, \ldots, \min\left(m, n\right)\]
              (Cette expression ne fait du sens que quand $\bvec{v}_i$ et $\bvec{u}_i$ sont définis, c'est pourquoi on a besoin du $\min$.)

              En effet, c'est vrai pour $i \leq r$ car c'est comme ça qu'on a défini $\bvec{u}_i$:
              \[\bvec{u}_i = \frac{1}{\left\|A \bvec{v}_i\right\|} = A \bvec{v}_i, \mathspace \sigma_i = \left\| A \bvec{v}_i\right\|\]

              De plus, c'est aussi vrai quand $i > r$ car $\sigma_i = 0$, donc:
              \[\left\|A \bvec{v}_i\right\| = 0 \implies A \bvec{v}_i = \bvec{0}\]


        \item Prenons les matrices suivantes, qui vont nous permettre de simplifier l'écriture:
              \[U = \begin{bmatrix}  &  &  \\ \bvec{u}_1 & \ldots & \bvec{u}_m \\  &  &  \end{bmatrix} \in \mathbb{R}^{m \times m}, \mathspace \text{orthogonale}\]
              \[V = \begin{bmatrix}  &  &  \\ \bvec{v}_1 & \ldots & \bvec{v}_n \\  &  &  \end{bmatrix} \in \mathbb{R}^{n \times n}, \mathspace \text{orthogonale}\]
              \[\Sigma \in \mathbb{R}^{m \times n}, \mathspace \text{ diagonale avec } \Sigma_{ii} = \sigma_i\]

              Pour rappel, une matrice $m \times n$ diagonale est telle que:
              \[i \neq j \implies \Sigma_{ij} = 0\]

              \imagehere{matriceDiagonaleSVD.png}

        \item On peut écrire nos équations de manières compactes:
              \[AV = U\Sigma\]

              Ainsi, puisque $VV^T = I_n$ (c'est une matrice orthogonale), on sait que $V^{-1} = V^T$. Ceci nous permet d'en déduire que:
              \[A = U\Sigma V^T\]

              On a donc bien trouvé la décomposition SVD de $A$.
    \end{itemize}
}

\parag{Décomposition en somme}{
    On sait que quand $A = A^T$, alors on peut écrire:
    \[A = VDV^T, \mathspace V = \begin{bmatrix}  &  &  \\ \bvec{v}_1 & \ldots & \bvec{v}_n \\  &  &  \end{bmatrix}, \mathspace D = \begin{bmatrix} \lambda_1 & \ldots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \ldots & \lambda_n \end{bmatrix} \]

    On avait discuté au cours précédent qu'on peut écrire cela sous la forme de la décomposition spectrale:
    \[A = \lambda_1 \bvec{v}_1 \bvec{v}_1^T + \ldots + \lambda_n \bvec{v}_n \bvec{v}_n^T\]

    De manière complètement similaire, on sait que, par la SVD, on peut écrire:
    \[A = U \Sigma V^T, \mathspace  \]

    Où:
    \[U = \begin{bmatrix}  &  &  \\ \bvec{u}_1 & \ldots & \bvec{u}_m \\  &  &  \end{bmatrix}, \mathspace V = \begin{bmatrix}  &  &  \\ \bvec{v}_1 & \ldots & \bvec{v}_n \\  &  &  \end{bmatrix}, \mathspace \Sigma = \begin{bmatrix} \sigma_1 & \ldots & 0 & \ldots & 0 \\ \vdots & \ddots & \vdots &  & \vdots \\ 0 & \ldots & \sigma_r & \ldots & 0 \\ \vdots &  & \vdots &  & \vdots \\ 0 & \ldots & 0 & \ldots & 0 \end{bmatrix}\]

    De manière très similaire, on peut donc aussi écrire:
    \[A = \sigma_1 \bvec{u}_1 \bvec{v}_1^T + \ldots + \sigma_r \bvec{u}_r \bvec{v}_r^T\]

    Ceci est justifiable de la même manière que pour la décomposition spectrale.
}

\end{document}
