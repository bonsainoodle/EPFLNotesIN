\documentclass[a4paper]{article}

% Expanded on 2022-05-24 at 15:18:36.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Mardi 24 mai 2022}

\begin{document}
\maketitle

\lecture{25}{2022-05-24}{Back to probabilities}{
}

\subsection{Decoding based on cosests and syndromes}

\parag{Equivalence relation}{
    Let $\left(G, \star\right)$ be a group and $\left(H, \star\right)$ be a subgroup.

    We define $a \sim b$, for $a, b \in G$, if $b = a \star h$ for some $h \in H$. The equivalence class of $a \in G$ is:
    \[\left[a\right] = \left\{b : a \sim b\right\} = \left\{a \star h, h \in H\right\} = a + H\]

    We call this the \important{coset} of $H$ with respect to $a$.

    \subparag{Properties}{
        \begin{enumerate}[left=0pt]
            \item Every $b \in G$ is in exactly one coset.

                  Indeed, if $b = a_1 \star h_1 = a_2 \star h_2$, then we get that:
                  \[a_1 = a_2 \star h_2 \star h_1^{-1} \implies a_1 \sim a_2\]
                  since $h_1, h_2 \in H \implies h_2 \star h_1^{-1} \in H$.
            \item All cosets have the same cardinality.
            \item Cosets are either equal or fully distinct.
            \item The union of all cosets is exactly $G$.
        \end{enumerate}

        In other words, cosets partition the set $G$ in equal shares.
    }
}

\parag{Generalisation: Standard Array}{
    Let's draw the following table, where each $\bvec{t_i}$ is chosen such that no two lines are the same:
    \begin{center}
        \begin{tabular}{c|c|ccc}
            $\bvec{x}$       & Coset of $\mathcal{C}$ with respect to $\bvec{x}$ & $D_0$                         & $\cdots$ & $D_{M-1}$                         \\
            \hline
            $\bvec{0}$       & $\mathcal{C}$                                     & $\bvec{c_0}$                  & $\cdots$ & $\bvec{c_{M-1}}$                  \\
            $\vdots$         & $\vdots$                                          & $\vdots$                      &          & $\vdots$                          \\
            $\bvec{t_{L-1}}$ & $\bvec{t_{L-1}} + \mathcal{C}$                    & $\bvec{t_{L-1}} + \bvec{c_0}$ & $\cdots$ & $\bvec{t_{L-1}} + \bvec{c_{M-1}}$
        \end{tabular}
    \end{center}

    We know $M = q^k$, since it is the size of our code. Also, since this table has every sequence exactly once and since $\mathbb{F}^n$ has $q^n$ elements, we want $ML = \left|\mathbb{F}^n\right| = q^n$, and thus $L = q^{n-k}$.

    Let's now define the set of elements in the $i$\Th column to be $D_i$, and call the $\bvec{t_i}$ the coset leaders. Such leaders are naturally not unique (as we have seen in the last example). In fact, we can pick any element of the line as its leader (a leader always belongs to its line since $\bvec{t_i} + \bvec{c_0} = \bvec{t_i}$).
}

\parag{Coset decoder}{
    Looking at our table from the last paragraph, if $\bvec{y} \in D_j$, then we output $\bvec{c_j}$. In other words, we return the first element of the column where $\bvec{y}$ lives.

    \subparag{More formal}{
        We receive $\bvec{y} \in \mathbb{F}_q^n$. We first identify the coset of $\bvec{y}$. This has a leader, let's call it $\bvec{t_i}$. Then, we output $\bvec{y} - \bvec{t_i}$.
    }
}

\parag{Theorem}{
    Choosing the leader of each coset to be one of the ones with the smallest Hamming weight, leads the coset decoder to be a minimum distance decoder.
}

\parag{Theorem}{
    Two $\bvec{y} \in \mathbb{F}^n$ have the same syndrome if and only if they are in the same coset.
}

\parag{Syndrome decoder}{
    We can sum up the syndrome decoder as the following algorithm.

    \begin{enumerate}
        \item Pre-compute and store the syndrome of each coset leader.
        \item Receive $\bvec{y} \in \mathbb{F}_q^n$, and compute its syndrome $\bvec{s} = \bvec{y} H^T$.
        \item Find $\bvec{t}$ by looking at the lookup table, knowing that its syndrome is also $\bvec{s}$.
        \item Output $\bvec{y} - \bvec{t}$.
    \end{enumerate}
}

\subsection{Error probability}

\parag{Binary symmetric channel}{
    Let's now add probabilities to our error channel. To do so, we will work over the field $\mathbb{F}_2$, and say that every bit has a flip probability $\epsilon$ (supposedly small). This is called the binary symmetric channel (BSC).
}

\parag{Linear codes}{
Now, let us use a linear code $\mathcal{C}$. Let's define $P_C\left(i\right)$ to be the probability that the Minimum-Distance decoder decodes correctly if the transmitted codeword is $\bvec{c_i}$.

We see that:
\[P_C\left(0\right) = P\left(\bvec{y} \in D_0 | \bvec{x} = \bvec{c_0}\right) = \sum_{\bvec{y} \in D_0}^{} P\left(\bvec{y} | \bvec{x} = \bvec{0}\right)= \sum_{\bvec{y} \in D_0}^{} \epsilon^{w\left(\bvec{y}\right)} \left(1- \epsilon\right)^{n - w\left(\bvec{y}\right)}\]

Now, let's compute it for a general $j$:
\[P_C\left(j\right) = P\left(\bvec{y} \in D_j | \bvec{x} = \bvec{c_j}\right) = P\left(\bvec{c_j} + \bvec{e} \in D_j | \bvec{x} = \bvec{c_j}\right)\]

However, we know $D_j = D_0 + \bvec{c_j}$, so:
\[\bvec{c}_j + \bvec{e} \in D_i \iff \bvec{e} \in D_0\]

This implies that that:
\[P_C\left(j\right) = P\left(\bvec{e} \in D_0 | \bvec{c_j}\right) = P\left(\bvec{e} \in D_0\right) = \sum_{\bvec{e} \in D_0}^{} \epsilon^{w\left(e\right)} \left(1 - \epsilon\right)^{n - w\left(\bvec{e}\right)}\]

However, this is exactly the sum we computed before, yielding:
\[P_C\left(j\right) = P_C\left(0\right) = \sum_{\bvec{y} \in D_0}^{} \left(\frac{\epsilon}{1 - \epsilon}\right)^{w\left(\bvec{y}\right)} \left(1 - \epsilon\right)^n, \mathspace \forall j\]
}

\parag{Lower bound}{
Computing such probability can be complicated, since we need to know $D_0$. Thus we need to have computed and stored the coset leaders which, as we have seen, is not a trivial task. Let us try to find an lower bound to this probability.

Let's say that our code has minimum distance $d_{min}$. We know that any error of weight $\left\lfloor \frac{d_{min} - 1}{2} \right\rfloor $ or less can be corrected. In other words, we know that any codeword with a weight less than or equal to $\left\lfloor \frac{d_{min} - 1}{2} \right\rfloor $ is a coset leader. Since there are $\binom{n}{i}$ possible codewords of length $n$ and weight $i$, we get our lower bound:
\[P_C\left(0\right) \geq \sum_{i=0}^{\left\lfloor \frac{d_{min} - 1}{2} \right\rfloor } \binom{n}{i} \epsilon^i \left(1 - \epsilon\right)^{n-i} = \sum_{i=0}^{\left\lfloor \frac{d_{min} - 1}{2} \right\rfloor } \binom{n}{i} \left(\frac{\epsilon}{1 - \epsilon}\right)^i \left(1 - \epsilon\right)^{n}\]

This is much easier to compute. Also, note that this is indeed an lower bound and not (always) an equality, since our code might be able to correct some errors of weight higher than $\left\lfloor \frac{d_{min} -  1}{2} \right\rfloor $ (but not all).
}

\end{document}
