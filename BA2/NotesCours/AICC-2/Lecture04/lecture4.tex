\documentclass[a4paper]{article}

% Expanded on 2022-03-03 at 08:19:46.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 03 mars 2022}

\begin{document}
\maketitle

\lecture{4}{2022-03-03}{Huffman codes}{
}

\parag{Advantage of prefix free code}{
    We will focus on prefix free codes because there is no loss of optimality. Indeed, codewords can be as short as for any uniquely decodable code, and a prefix-free codeword is recognised as soon as its last digit is seen.
}

\parag{Definition: Average Codeword Length}{
    Let $\ell \left(\Gamma\left(s\right)\right)$ be the length of the codeword associated to $s \in \mathcal{A}$. The \important{average codeword-length} is:
    \[L\left(S, \Gamma\right) = \sum_{s \in \mathcal{A}}^{} p_S\left(s\right) \ell \left(\Gamma\left(s\right)\right)\]


    \subparag{Remark}{
        The unit of this value is code symbols. Thus, if $D = 2$, then it is bits, and it would represent the average number of bits required on a hard drive.
    }
}

\parag{Theorem}{
    Let $\Gamma : \mathcal{A} \mapsto \mathcal{C}$ be the encoding map of a $D$-ary code for the random variable $S \in \mathcal{A}$.

    If the code is uniquely decodable, then:
    \[H_{D}\left(S\right) \leq L\left(S, \Gamma\right)\]
    with equality if and only if $p_s\left(s\right) = D^{-\ell\left(\Gamma\left(s\right)\right)}$ for all $s \in \mathcal{A}$.

    Note that an equivalent condition is $\ell\left(\Gamma\left(s\right)\right) = \log_D\left(\frac{1}{p_S\left(s\right)}\right)$ for all $s \in \mathcal{A}$
}

\parag{Observation}{
    We notice that we defined:
    \[L\left(S, \Gamma\right) = \sum_{s \in A}^{} p\left(s\right) \ell\left(\Gamma\left(s\right)\right)\]
    \[H_D\left(S\right) = \sum_{s \in A}^{} p\left(s\right) \log_D\left(\frac{1}{p_S\left(s\right)}\right)\]

    Thus, they are identical if $\ell\left(\Gamma\left(s\right)\right) = \log_D\left(\frac{1}{p_S\left(s\right)}\right)$, just like we have seen with our theorem. Unfortunately, we will often not be able to choose lengths such that this is respected, because we want an integer.
}

\parag{Theorem}{
    For every random variable $S \in \mathcal{A}$ and every integer $D \geq 2$, there exists a prefix-free $D$-ary code for $S$ such that, for all $s \in \mathcal{A}$:
    \[\ell\left(\Gamma\left(s\right)\right) = \left\lceil \log_D\left(\frac{1}{p_S\left(s\right)}\right) \right\rceil\]

    Such codes are called \important{$D$-ary Shannon-Fano codes}.
}

\parag{Theorem}{
    The average codeword-length of a $D$-ary Shannon-Fano code for the random variable $S$ fulfills:
    \[H_D\left(S\right) \leq L\left(S, \Gamma_{SF}\right) < H_D\left(S\right) + 1\]
}

\parag{Definition: Tree with probabilities}{
    Let's consider a tree with probabilities assigned to leaf nodes, like decoding tree of a prefix-free code. The probabilities of the leaf nodes induce probabilities to the intermediate nodes (like in Huffman's construction). The result is called a \important{tree with probabilities} (the two pictures above are such trees).
}

\parag{Path-length lemma}{
    The average path length of a tree with probabilities is the sum of probabilities of the intermediate nodes (root included):
    \[\sum_{i}^{} p_i \ell_i = \sum_{j}^{} q_j\]
    where $p_i$ is the probability of the $i$\Th leaf, $\ell_i$ the distance of the $i$\Th leaf, and $q_j$ the probability of the $j$\Th node (except for leafs).
}

\parag{Theorem: Huffman Code optimality}{
    Huffman code are optimal. In other words, if $\Gamma_H$ is a Huffman code (which is prefix-free by construction) and $\Gamma$ is another uniquely decodable code for the same source $S$, then it is guaranteed that:
    \[L\left(S, \Gamma_H\right) \leq L\left(S, \Gamma\right)\]
}

\end{document}
