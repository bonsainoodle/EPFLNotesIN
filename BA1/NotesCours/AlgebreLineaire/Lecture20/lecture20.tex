\documentclass[a4paper]{article}

% Expanded on 2021-11-29 at 13:15:27.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Lundi 29 novembre 2021}

\begin{document}
\maketitle

\lecture{20}{2021-11-29}{Kernel, image, famille et espace orthogonal}{
\begin{itemize}[left=0pt]
    \item Démonstrations que l'orthogonal du kernel est l'espace engendré par les lignes, et explication similaire pour les trois autres espaces d'une matrice.
    \item Définition de famille orthogonale, et démonstration qu'une famille orthogonale avec aucun vecteur nul est libre.
    \item Définition de base orthogonale, et explication de la méthode pour trouver les coefficients d'un vecteur dans une base orthogonale.
    \item Explication des projections orthogonales en 2D.
\end{itemize}

}

\parag{Observation}{
    Soient les trois vecteurs suivants:
    \[\bvec{v}_1 = \begin{bmatrix} 7 \\ 3 \\ 1 \end{bmatrix}, \mathspace \bvec{v}_2 = \begin{bmatrix} 2 \\ 4 \\ 5 \end{bmatrix}, \mathspace \bvec{x} = \begin{bmatrix} 9 \\ 10 \\ 8 \end{bmatrix} \]

    Alors, on voit que:
    \[\begin{bmatrix}  & \bvec{v}_1^T &  \\  & \bvec{v}_2^T &  \end{bmatrix} \bvec{x} = \begin{bmatrix} 7 & 3 & 1 \\ 2 & 4 & 5 \end{bmatrix} \begin{bmatrix} 9 \\ 10 \\ 8 \end{bmatrix} = \begin{bmatrix} 7\cdot 9 + 3\cdot 10 + 1\cdot 8 \\ 2\cdot 9 + 4\cdot 10 + 5\cdot 8 \end{bmatrix} = \begin{bmatrix} \bvec{v}_1 \dotprod \bvec{x} \\ \bvec{v}_2 \dotprod \bvec{x} \end{bmatrix} \]

    Ceci peut être démontré pour un cas complètement général.
}

\parag{Retour aux quatre sous-espaces}{
    Pour une matrice $A \in \mathbb{R}^{m \times n}$, on a quatre sous-espaces:
    \begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \fullbf{Sous espaces de $\mathbb{R}^n$} & \fullbf{Sous espaces de $\mathbb{R}^m$}  \\
        \hline
        $\ker\left(A\right)$ & $\im\left(A\right)$ \\
        $\im\left(A^T\right) = \lgn\left(A\right)$ & $\ker\left(A^T\right)$ \\
        \hline
    \end{tabular}
    \end{center}

    On avait déjà vu que:
    \[\dim\left(\ker A\right) + \dim\left(\im A^T\right) = \dim \mathbb{R}^n\]
    \[\dim\left(\im A\right) + \dim\left(\ker A^T\right) = \dim \mathbb{R}^m\]
}

\parag{Théorème}{
    Soit une matrice $A$ quelconque. On a:
    \[\left(\im\left(A^T\right)\right)^{\perp} = \ker A\]

    \subparag{Preuve de $\supseteq$}{
        On veut montrer que $\ker A \subseteq \left(\im\left(A^T\right)\right)^{\perp}$.

        \vspace{1em}

        Soit $\bvec{x}$ un vecteur quelconque dans $\ker A$, donc:
        \[A \bvec{x} = \bvec{0}.\]

        Soit $\bvec{w}$ un vecteur quelconque dans $\im\left(A^T\right)$, donc:
        \[\exists \bvec{c} \telque \bvec{w} = A^T \bvec{c}\]

        On doit vérifier que $\bvec{x}$ et $\bvec{w}$ sont orthogonaux:
        \[\bvec{w} \dotprod \bvec{x} = \bvec{w}^T \bvec{x} = \left(A^T \bvec{c}\right)^T \bvec{x} = \bvec{c}^T A \bvec{x} = \bvec{c}^T \bvec{0} = \bvec{0}\]

        Ainsi, on en déduit bien que $\bvec{x}$ est orthogonal à $\bvec{w}$, donc que $\bvec{x} \in \left(\im A^T\right)^\perp$.
    }

    \subparag{Preuve de $\subseteq$}{
        On veut montrer que $\left(\im\left(A^T\right)\right)^{\perp} \subseteq \ker A $.

        \vspace{1em}

        Soit $\bvec{x}$ un vecteur quelconque de $\left(\im\left(A^T\right)\right)^\perp$, donc $\bvec{x}$ est orthogonal à tous les vecteurs de $\im\left(A^T\right)$. En particulier, $\bvec{x}$ est orthogonal à chaque colonne de $A^T$, c'est-à-dire à chaque ligne de $A$.

        Comme ce qu'on a vu plus haut, la $i$-ème entrée de $A \bvec{x}$ est le produit scalaire de la $i$-ème ligne de $A$ avec $\bvec{x}$, ce qui est à chaque fois nul puisque $\bvec{x}$ est orthogonal à tous les vecteurs de $\im\left(A^T\right)$. On en déduit donc que $A \bvec{x} = \bvec{0}$.

        Ainsi, $\left(\im A\right)^\perp \subseteq \ker A$.

        \qed
    }
}

\parag{Corollaire}{
    Soit $A$ une matrice. On a:
    \begin{itemize}
        \item $\left(\ker A\right)^{\perp} = \im\left(A^T\right)$
        \item $\left(\im A\right)^\perp = \ker\left(A^T\right)$
        \item $\left(\ker A^T\right)^\perp = \im\left(A\right)$
        \item $\left(\im A^T\right)^{\perp} = \ker\left(A\right)$
    \end{itemize}

    On remarque que pour chaque égalité on a échangé $\ker$ et $\im$, on a échangé $A$ et $A^T$, et on a échangé $V$ et $V^\perp$.

    \subparag{Intuition}{
        Toutes les personnes à qui j'ai demandé et tous les liens que j'ai consulté sur internet sortaient simplement la démonstration du théorème ci-dessus comme intuition. Il va donc falloir s'en contenter.
    }

    \subparag{Preuve}{
        Puisqu'on sait que $\ker A = \left(\im\left(A^T\right)\right)^\perp$, on a:
        \[\left(\ker A\right)^\perp = \left(\im\left(A^T\right)^\perp\right)^\perp = \im\left(A^T\right)\]

        De plus, en appliquant ces relations à $A^T$ au lieu de $A$ (puisque le théorème ci-dessus est vrai pour toutes les matrices, il l'est aussi pour $A^T$), on a:
        \[\ker\left(A^T\right) = \left(\im\left(\left(A^T\right)^T\right)\right)^\perp = \left(\im A\right)^\perp\]
    \[\left(\ker A^T\right)^\perp = \left(\left(\im A\right)^\perp\right)^{\perp} = \im A\]
    }

}


\parag{Théorème d'Al Kashi}{
    On sait que, dans n'importe quel triangle:
    \[a^2 + b^2 - 2ab \cos\left(\gamma\right) = c^2\]
    où $\gamma$ est l'angle opposé au côté $c$ (donc l'angle entre le côté $a$ et le côté $b$).

    Ce théorème est appelé le théorème d'Al Kashi, le théorème des cosinus ou, par certains hérétiques, le théorème de Pythagore généralisé.

    \subparag{Implication}{
        De manière générale, deux vecteur dessinent un triangle:

        \imagehere[0.4]{AlKashiProduitScalaire.png}

        Ainsi, on trouve par Al Kashi que:
        \begin{multiequality}
        & \left\|\bvec{u}\right\|^2 + \left\|\bvec{v}\right\|^2 - 2\left\|\bvec{u}\right\|\left\|\bvec{v}\right\|\cos \theta \\
        =\ & \left\|\bvec{u} - \bvec{v}\right\|^2  \\
        =\ & \left(\bvec{u} - \bvec{v}\right)\dotprod\left(\bvec{u} - \bvec{v}\right)  \\
        =\ & \bvec{u}\dotprod \bvec{u} - \bvec{u} \dotprod \bvec{v} - \bvec{v} \dotprod \bvec{u} + \bvec{v} \dotprod \bvec{v}  \\
        =\ & \left\|\bvec{u}\right\|^2 + \left\|\bvec{v}\right\|^2 - 2 \bvec{u} \dotprod \bvec{v}
        \end{multiequality}

        On peut donc en déduire que:
        \[\bvec{u} \dotprod \bvec{v} = \left\|\bvec{u}\right\| \left\|\bvec{v}\right\| \cos \theta\]

        Ainsi, on remarque bien que deux vecteurs non-nuls sont orthogonaux si et seulement si le cosinus de l'angle entre les deux est nul, donc si et seulement s'ils sont perpendiculaires.
    }
}

\subsection{Familles orthogonales}
\parag{Définition}{
    Des vecteurs $\left(\bvec{u}_1, \ldots, \bvec{u}_p\right)$ de $\mathbb{R}^n$ forment une \important{famille orthogonale} si chaque vecteur est orthogonal à tous les autres, c'est-à-dire si:
    \[\bvec{u}_i \dotprod \bvec{u}_j = 0, \mathspace \forall i \forall j, i \neq j\]

    \subparag{Remarque}{
        Comme $\bvec{0} \dotprod \bvec{u} = \bvec{0}$ pour tout $\bvec{u}$, on peut très bien avoir des vecteurs nuls dans une famille orthogonale.

        Cependant, on est souvent plus intéressé par des familles de vecteurs non-nuls à cause du théorème qui va suivre après ces exemples.
    }
}

\parag{Exemple 1}{
    Soient les vecteurs de $\mathbb{R}^2$ suivants:
    \[\bvec{u}_1 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}, \mathspace \bvec{u}_2 = \begin{bmatrix} 2 \\ -4 \end{bmatrix} \]

    La famille $\left(\bvec{u}_1, \bvec{u}_2\right)$ est orthogonale car:
    \[\bvec{u}_1 \dotprod \bvec{u}_2 = 2\cdot 2 + 1\left(-4\right) = 4 - 4 = 0\]
}

\parag{Exemple 2}{
    Soient les trois vecteurs de $\mathbb{R}^3$ suivants:
    \[\bvec{u}_1 = \begin{bmatrix} 3 \\ 1 \\ 1 \end{bmatrix}, \mathspace \bvec{u}_2 = \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}, \mathspace \bvec{u}_3 = \begin{bmatrix} -1 / 2 \\ -2 \\ 7 / 2 \end{bmatrix} \]

    La famille $\left(\bvec{u}_1, \bvec{u}_2, \bvec{u}_3\right)$ est orthogonale car:
    \[\bvec{u}_1 \dotprod \bvec{u}_2 = 3\left(-1\right) + 1\left(2\right) + 1\left(1\right) = 0\]
    \[\bvec{u}_1 \dotprod \bvec{u}_3 = 3\left( \frac{-1}{2}\right) + 1\left(-2\right) + 1\left(\frac{7}{2}\right) = 0\]
    \[\bvec{u}_2 \dotprod \bvec{u}_3 = -1\left(\frac{-1}{2}\right) + 2\left(-2\right) + 1\left(\frac{7}{2}\right) = 0\]

}

\parag{Théorème}{
    Si les vecteurs $\left(\bvec{u}_1, \ldots, \bvec{u}_p\right)$ sont \textit{tous} non-nuls et forment une famille orthogonale, alors ils sont linéairement indépendants.

    \subparag{Preuve}{
        Considérons des coefficients $c_1, \ldots, c_p$ quelconques tels que:
        \[\bvec{0} = c_1 \bvec{u}_1 + \ldots + c_p \bvec{u}_p\]

        On doit montrer qu'ils sont tous nuls. Commençons par montrer que $c_1 = 0$. On remarque que:
        \begin{multiequality}
        0 =\ & \bvec{0} \dotprod \bvec{u}_1  \\
        =\ & \left(c_1 \bvec{u}_1 + \ldots + c_p \bvec{u}_p\right) \dotprod \bvec{u}_1 \\
        =\ & c_1 \left(\bvec{u}_1 \dotprod \bvec{u}_1\right) + c_2 \left(\underbrace{\bvec{u}_2 \dotprod \bvec{u}_1}_{= 0}\right) + \ldots + c_p \left(\underbrace{\bvec{u}_p \dotprod \bvec{u}_1}_{= 0}\right)  \\
        =\ & c_1 \left\|\bvec{u}_1\right\|^2
        \end{multiequality}

        Puisque $\bvec{u}_1$ est non-nul, $\left\|\bvec{u}_1\right\|^2 \neq 0$. Ainsi, nécessairement, $c_1 = 0$.

        On peut répéter le même raisonnement pour montrer que $c_2 = 0, \ldots, c_p = 0$. Donc, la seule combinaison linéaire de $\bvec{u}_1, \ldots, \bvec{u}_p$ qui est égale à $\bvec{0}$ est la combinaison triviale, ce qui implique que les vecteurs sont libres.

        \qed
    }

}

\parag{Définition}{
    Soit $\left(\bvec{u}_1, \ldots, \bvec{u}_p\right)$ une base d'un sous-espace $W$ de $\mathbb{R}^n$. Si cette famille est orthogonale, on dit qu'elle forme une \important{base orthogonale} de $W$.

    \subparag{Utilité}{
        Les bases orthogonales sont très pratiques. En effet, une des raisons est qu'il est très facile de trouver les coordonnées dans ce type de base.
    }
}

\parag{Théorème}{
    Soit $\left(\bvec{u}_1, \ldots, \bvec{u}_p\right)$ une base orthogonale d'un sous-espace vectoriel $W$ de $\mathbb{R}^n$. Pour tout $\bvec{y}$ de $W$, les coefficients de la combinaison linéaire $\bvec{y} = c_1 \bvec{u}_1 + \ldots + c_p \bvec{u}_p$ sont données par la relation:
    \[c_j = \frac{\bvec{y} \dotprod \bvec{u}_j}{\bvec{u}_j \dotprod \bvec{u}_j}, \mathspace j = 1, \ldots, p\]

    \subparag{Mnémotechnie}{
        Notez qu'il n'est pas nécessaire d'apprendre cette formule par cœur, il est plus simple de comprendre la démonstration qui suit et de redériver cette formule à chaque fois (même le professeur la redérive à chaque fois, on voit bien qu'on est pas des étudiants en biologie vu la quantité d'appris par cœur).
    }

    \subparag{Implication}{
        On peut donc facilement trouver $\left[\bvec{y}\right]_{\mathcal{B}}$:
        \[\left[\bvec{y}\right]_{\mathcal{B}} = \begin{bmatrix} \frac{\bvec{y} \dotprod \bvec{u}_1}{\left\|\bvec{u}_1\right\|^2} \\ \vdots \\ \frac{\bvec{y} \dotprod \bvec{u}_n}{\left\|\bvec{u}_n\right\|^2} \end{bmatrix} \]
    }

    \subparag{Preuve}{
        Soit $\bvec{y} \in W$ quelconque.

        Comme $\mathcal{B} = \left(\bvec{u}_1, \ldots, \bvec{u}_p\right)$ est une base de $W$, on sait qu'il existe un choix unique de coefficients $c_1, \ldots, c_p$ tels que:
        \[\bvec{y} = c_1 \bvec{u}_1 + \ldots + c_p \bvec{u}_p\]

        Calculons $c_2$, par exemple:
        \begin{multiequality}
        \bvec{y} \dotprod \bvec{u}_2 =\ & \left(c_1 \bvec{u}_1 + \ldots + c_p \bvec{u}_p\right) \dotprod \bvec{u}_2  \\
        =\ & c_1 \left(\bvec{u}_1 \dotprod \bvec{u}_2\right) + c_2 \left(\bvec{u}_2 \dotprod \bvec{u}_2\right) + \ldots + c_p\left(\bvec{u}_p \dotprod \bvec{u}_2\right)  \\
        =\ & c_2 \bvec{u}_2 \dotprod \bvec{u}_2
        \end{multiequality}

        Donc, on trouve:
        \[c_2 = \frac{\bvec{y} \dotprod \bvec{u}_2}{\bvec{u}_2 \dotprod \bvec{u}_2}\]

        Ce qui est bien défini puisque $\bvec{u}_2 \neq \bvec{0}$.

        On peut faire le même raisonnement pour n'importe quel vecteur de la base.

        \qed
    }

}

\parag{Exemple}{
    Soient les trois vecteurs de $\mathbb{R}^3$ suivants:
    \[\bvec{u}_1 = \begin{bmatrix} 3 \\ 1 \\ 1 \end{bmatrix}, \mathspace \bvec{u}_2 = \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}, \mathspace \bvec{u}_3 = \begin{bmatrix} -1 / 2 \\ -2 \\ 7 / 2 \end{bmatrix} \]

    On a déjà vérifié que la famille $\left(\bvec{u}_1, \bvec{u}_2, \bvec{u}_3\right)$ est orthogonale. Comme les vecteurs sont tous non-nuls, on sait qu'ils forment une base de pour $\vect\left\{\bvec{u}_1, \bvec{u}_2, \bvec{u}_3\right\} = \mathbb{R}^3$.

    Prenons le vecteur suivant de $\mathbb{R}^3$:
    \[\bvec{y} = \begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix} \]

    On peut trouver ses coordonnées dans la base $\mathcal{B} = \left(\bvec{u}_1, \bvec{u}_2, \bvec{u}_3\right)$:
    \[c_1 = \frac{\bvec{y} \dotprod \bvec{u}_1}{\bvec{u}_1 \dotprod \bvec{u}_1} = \frac{1\left(3\right) + 1\left(1\right) + 2\left(1\right)}{3\left(3\right) + 1\left(1\right) + 1\left(1\right)} = \frac{6}{11}\]

    On peut faire le même raisonnement pour les autres coordonnées.
}

\subsection{Projections orthogonales}
\parag{En 2D}{
    Considérons une droite qui passe par l'origine $W$ (c'est un sous-espace vectoriel) et un point $\bvec{y}$ dans $\mathbb{R}^2$.
    On se demande quel est le point $\bvec{\hat{y}}$ sur $W$ qui est le plus proche de $\bvec{y}$.

    \imagehere[0.5]{DistancePlusProcheEstOrthogonale.png}

    On se rend compte que le point le plus proche est celui projeté orthogonalement sur $W$. En effet, peu importe le point $\bvec{w} \neq \bvec{\hat{y}}\in W$ qu'on prend cela dessine un triangle rectangle entre $\bvec{w}$, $\bvec{y}$ et $\bvec{\hat{y}}$, donc la distance entre $\bvec{w}$ et $\bvec{y}$ (l'hypothénuse du triangle rectangle) est forcément plus grande que celle entre $\bvec{y}$ et $\bvec{\hat{y}}$ (une des cathètes de ce triangle).

    On prend $\bvec{z}$ tel que $\bvec{y} = \bvec{\hat{y}} + \bvec{z}$. Puisque $\bvec{\hat{y}}$ est sur la droite, on voit que $\bvec{\hat{y}} = c \bvec{u}$. De plus, $\bvec{z} \dotprod \bvec{u} = 0$ puisqu'ils sont orthogonaux. Ainsi, on en déduit que:
    \[0 = \bvec{z} \dotprod \bvec{u} = \left(\bvec{y} - \bvec{\hat{y}}\right) \dotprod \bvec{u} = \left(\bvec{y} - c \bvec{u}\right) \dotprod \bvec{u} = \bvec{y} \dotprod \bvec{u} - c \bvec{u} \dotprod \bvec{u}\]

    Ainsi, on en déduit que:
    \[\bvec{y} \dotprod \bvec{u} = c \bvec{u} \dotprod \bvec{u} \implies c = \frac{\bvec{y} \dotprod \bvec{u}}{\bvec{u} \dotprod \bvec{u}}\]

    Ce qui nous permet de trouver la formule pour $\bvec{\hat{y}}$:
    \[\bvec{\hat{y}} = \frac{\bvec{y} \dotprod \bvec{u}}{\bvec{u} \dotprod \bvec{u}} \bvec{u}\]
}

\end{document}
