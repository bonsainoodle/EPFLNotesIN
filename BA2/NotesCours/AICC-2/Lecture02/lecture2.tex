\documentclass[a4paper]{article}

% Expanded on 2022-02-24 at 08:18:12.

\usepackage{../../style}

\title{AICC 2}
\author{Joachim Favre}
\date{Jeudi 24 fÃ©vrier 2022}

\begin{document}
\maketitle

\lecture{2}{2022-02-24}{IT inequality}{
}

\parag{Definition: Independent random variables}{
    Two random variables $X$ and $Y$ are \important{independent} if and only if, for all realisations $x$ and $y$:
    \[p\left(\left\{X = x\right\} \cap \left\{Y = y\right\}\right) = p\left(\left\{X = x\right\}\right) p\left(\left\{Y = y\right\}\right)\]

    More concisely, if and only if:
    \[p_{X, Y}\left(x, y\right) = p_X\left(x\right)p_Y\left(y\right) \]

    We can generalise this to $n$ random variables. The random variables $X_1, \ldots, X_n$ are independent if and only if:
    \[p_{X_1, \ldots, X_n}\left(x_1, \ldots, x_n\right) = \prod_{i=1}^{n} p_{X_i}\left(x_i\right)\]
}

\parag{Definition: Conditional distribution}{
    The \important{conditional distribution} of $Y$ given $X$ is the function $p_{Y | X}$ defined by:
    \[p_{Y|X}\left(y | x\right) = \frac{p_{X, Y}\left(x, y\right)}{p_{X\left(x\right)}}\]
}

\parag{Independence}{
    All the following sentences are equivalent:
    \begin{itemize}
        \item $X$ and $Y$ are independent.
        \item $\displaystyle p_{X, Y} = p_{X} p_Y$
        \item $\displaystyle p_{Y | X}\left(y | x\right) = p_Y\left(y\right)$
        \item $\displaystyle p_{X | Y}\left(x|y\right) = p_{X}\left(x\right)$
    \end{itemize}
}

\parag{Product of expected values}{
    If $X$ and $Y$ are independent, then:
    \[\exval\left[XY\right] = \exval\left[X\right] \exval\left[Y\right]\]
}

\subsection{Entropy}

\parag{Entropy}{
    We finally come up with the definition of \important{entropy}, the one Shannon defined:
    \[H_b\left(S\right) = - \sum_{s \in \supp\left(p_S\right)}^{} p_S\left(s\right) \log_b\left(p_S\left(s\right)\right), \mathspace \text{where } \supp\left(p_S\right) = \left\{s: p_S\left(s\right) > 0\right\}\]

    We need the $\supp\left(p_S\right)$, the support of $p_S$, since $\log\left(0\right)$ is undefound (mistake made on purpose, we have to be coherent with our incoherences). To simplify the notation, we can also declare that $p_S\left(s\right) \log\left(p_S\left(s\right)\right) = 0$ when $p_S\left(s\right) = 0$ (this is coherent since $x \log\left(x\right) \to 0$ when $x \to 0$). This convention allows us to simplify the notation to:
    \[H_b\left(S\right) = -\sum_{s \in A}^{} p_S\left(s\right) \log_b \left(p_S\left(s\right)\right)\]

    We can also see it the following way:
    \[H\left(S\right) = \exval\left[-\log\left(p_S\left(S\right)\right)\right]\]

    \subparag{Unit}{
        The choice of the base $b$ determines the unit. Typically, $b = 2$ leads to bits, and much less common, $b = e$ leads to nats.
    }
}

\parag{Uniform distribution}{
    Let $p_S\left(s\right)$ be a uniform distribution, i.e.:
    \[p_S\left(s\right) = \frac{1}{\left|A\right|}\]

    Then,we can compute the entropy:
    \[H_b\left(S\right) = -\sum_{s \in A}^{} \frac{1}{\left|A\right|} \log_b\left(\frac{1}{\left|A\right|}\right) = \log_b\left|A\right| \underbrace{\sum_{s \in A}^{} \frac{1}{\left|A\right|}}_{= 1} = \log_b \left|A\right|\]
}

\parag{Binary entropy function}{
    Let's consider the special case when $\left|A\right| = 2$.

    In this case, $p_S$ has only two possible values, let's say $p$ and $\left(1 - p\right)$. This corresponds to the entropy:
    \[H\left(S\right) = h\left(p\right) = -p\log_2\left(p\right) - \left(1 - p\right) \log_2\left(1 - p\right)\]
}

\parag{Lemma: IT-inequality}{
    Let $r \in \mathbb{R}_+$. Then:
    \[\log_b\left(r\right) \leq \left(r - 1\right) \log_b\left(e\right)\]
    with equality if and only if $r = 1$.

    In other words, the function $\log_b\left(r\right)$ is under its tangent at $r = 1$ (which equation is $y = \left(r - 1\right) \log_b\left(e\right)$).
}

\parag{Theorem: Entropy bound}{
    The entropy of a discrete random variable $S \in A$ satisfies:
    \[0 \leq H_b\left(S\right) \leq \log_b\left|A\right|\]
    with equality on the left if and only if $p_S\left(s\right) = 1$ for some $s$ (certainty), and with equality on the right if and only if $p_S\left(s\right) = \frac{1}{\left|A\right|}$ for all $s$ (uniform distribution).
}

\parag{Two random variables}{
    We can also define entropy for two random variables:
    \[H\left(X, Y\right) = -\sum_{\left(x, y\right) \in \mathcal{X} \times \mathcal{Y}}^{} p_{X,Y}\left(x, y\right) \log\left(p_{X,Y}\left(x,y\right)\right)\]

    We can also define it using expected values:
    \[H\left(X, Y\right) = \exval\left[-\log\left(p_{X,Y}\left(X, Y\right)\right)\right]\]
}

\parag{Sequences of random variables}{
We want to compress large amounts of data, thus we can have long sequences of random variables. It can be finite, $\left(S_1, \ldots, S_n\right)$, infinite, $\left\{S_i\right\}_{i=1}^{\infty}$ (we can also consider $\ldots, S_{-1}, S_{0}, S_{1}, \ldots$ written $\left\{S_i\right\}$).

A collection of random variables $\left(S_1, \ldots S_n\right)$ is specified by the joint probability distribution $p_{S_1, \ldots S_n}$. This is all we need to compute the entropy $H\left(S_1, \ldots, S_n\right)$.
}

\parag{Theorem}{
    Let $S_1, \ldots S_n$ be discrete random variables. Then:
    \[H\left(S_1, \ldots, S_n\right)\leq H\left(S_1\right) + \ldots+ H\left(S_n\right)\]
    with equality if and only if $S_1, \ldots, S_n$ are independent.
}

\end{document}
